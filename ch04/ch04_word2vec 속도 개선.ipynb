{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3장의 simple word2vec에 두 가지 개선 방안 추가**\n",
    "\n",
    "1. Embedding이라는 새로운 계층 도입\n",
    "\n",
    "2. 네거티브 샘플링이라는 새로운 손실 함수 도입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# word2vec 개선 (1)\n",
    "\n",
    "3장에서의 CBOW 모델은 다루는 어휘의 수가 총 7개이기에 문제없이 작동한다. 하지만 거대한 말뭉치를 다루게 되면 문제가 발생한다.  \n",
    "\n",
    "어휘가 100만 개, 은닉층의 뉴런이 100개인 CBOW를 생각해보자. 입력층과 출력층에는 각 100만 개의 뉴런이 존재한다.\n",
    "\n",
    "#### 이 때 다음의 두 가지 병목현상이 발생한다.\n",
    "\n",
    "\n",
    "- **입력층의 원핫 표현과 가중치 행렬 $W_{in}$ 의 곱 계산**\n",
    "  - 원핫 표현과 관련된 문제\n",
    "  - `Embedding`이라는 새로운 계층 도입\n",
    "  \n",
    "        \n",
    "- **은닉층과 가중치 행렬 $W_{out}$의 곱 및 Softmax 계층의 계산**\n",
    "  - 은닉층 이후의 계산 문제\n",
    "  - 은닉층과 가중치 행렬 $W_{out}$의 곱의 계산량이 상당함\n",
    "  - `negative sampling` 이라는 새로운 손실 함수 도입\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 계층\n",
    "\n",
    "어휘의 수가 100만 개, 은닉층의 뉴런이 100개인 경우를 가정해보자.\n",
    "\n",
    "$c * W_{in} = h$ 일 때, 형상은 다음과 같다.\n",
    "\n",
    "- c.shape = (1, 1000000)\n",
    "- W.shape = (1000000, 100)\n",
    "- h.shape = (1,100)\n",
    "\n",
    "**> 이는 결과적으로 $W_{in}$ 행렬에서 특정 행을 추출하는 것 뿐이다.**   \n",
    "여기서 말하는 \"특정 행\"이라 함은 $W_{in}$ 행렬에서 input c 단어의 id에 해당하는 인덱스 행을 말함\n",
    "\n",
    "### 즉, 원핫 표현과 MatMul 계층의 행렬곱 연산은 사실 상 불필요한 작업임!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 가중치 매개변수로부터 `단어 ID에 해당하는 행(벡터)`를 추출하는 계층을 만들어보자!<br>\n",
    "<u>여기서의 계층이 바로 Embedding 계층!</u>\n",
    "\n",
    "**참고) Embedding은 단어 임베딩이라는 용어에서 유래했다.\n",
    "\n",
    "즉, 우리가 할 일은 Embedding 계층에 단어 임베딩 (분산 표현)을 저장하는 것이다.\n",
    "\n",
    "`Q?` 기존의 W에 저장될 분산 표현이 Embedding 계층으로 옮겨간 것?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([15, 16, 17])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 단일 행 뽑기\n",
    "# idx = 2\n",
    "display(W[2])\n",
    "# idx = 5\n",
    "display(W[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5],\n",
       "       [ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idx 여러개 한번에 추출\n",
    "idx = np.array([1,0,3,0])\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding: \n",
    "    def __init__(self,W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None # 단어 ID 인덱스를 배열로 저장, 미니배치를 고려할 때 여러개도 추출될 수 있도록 구현\n",
    "        \n",
    "    # 가중치 W의 특정 행을 추출\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx] \n",
    "        return out\n",
    "    \n",
    "    # 반대로 역전파에서는 반대로 출력층에서의 기울기를 W의 idx 행에 할당\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads # 가중치 기울기 dW를 꺼낸다\n",
    "        dW[...] = 0 # dW의 형상을 유지한 채 원소만 0으로 덮어쓴다\n",
    "        \n",
    "        # idx 중복을 고려하지 않은 방법\n",
    "        # dW[self.idx] = dout # 앞 층에서 전해진 기울기 dout을 idx 번째 행에 할당한다. \n",
    "        \n",
    "        # idx 중복 고려 : 할당 x 더하기 !\n",
    "        # Method #1 느리당\n",
    "        #for i, word_id in enumerate(self.idx):\n",
    "        #    dW[word_id] += dout[i]\n",
    "        \n",
    "        # Method #2 효율적. np.add.at(A,idx,B) = B를 A의 idx번째 행에 더해준다.\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[주의]** 역전파 과정에서 idx의 중복을 고려하여 값을 '할당'하는 것이 아니라 값을 **더하기** 한다.\n",
    "\n",
    "### 단점\n",
    " - 다의어 문제 해결 불가\n",
    " - 단어의 위치 고려가 되지 않음\n",
    " \n",
    "**-> 궁극적으로 문장 단위로 가야 하는 이유 (Elmo, Transformer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding 계층 구현으로 인한 이점**\n",
    "\n",
    ": 입력 측 MatMul 계층을 Embedding 계층으로 전환하여 메모리 사용량을 줄이고 쓸데없는 계산량을 생략!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec 개선 (2)\n",
    "\n",
    "은닉층 이후, <u>행렬 곱과 Softmax 계층의 계산</u>에서 발생하는 병목 현상을 해결해보자\n",
    "\n",
    "by **`네거티브 샘플링 기법`**\n",
    "\n",
    "Softmax 대신 네거티브 샘플링을 이용하면 어휘가 아무리 많아져도 계산량을 억제할 수 있다.\n",
    "\n",
    "어렵다! 복잡하다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 은닉층 이후 계산의 문제점\n",
    "\n",
    "마찬가지로 어휘가 100만 개, 은닉층 뉴런의 수가 100개인 CBOW 모델을 가정해보자\n",
    "\n",
    "<img src=\"../imgs/fig 4-2.png\" width=\"500\" align='left'>\n",
    "<br></br>\n",
    "\n",
    "**문제점 1. 은닉층의 뉴런 X $W_{out}$**\n",
    "\n",
    "    - 은닉층 벡터의 크기가 100\n",
    "    - 가중치 행렬의 크기는 100 X 100만\n",
    "<br>\n",
    "\n",
    "**문제점 2. Softmax 계층의 계산**\n",
    "    \n",
    "    - Softmax의 계산식에서 분모의 값을 얻기 위해 Exponential 계산을 100만 번 수행해야 한다...!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 분류에서 이진 분류로\n",
    "\n",
    "네거티브 샘플링 기법의 핵심 아이디어는 `이진 분류 (binary classification)` 에 있다.   \n",
    "\n",
    "즉, **<u>다중 분류를 이진 분류로 근사하는 것</u>**\n",
    "\n",
    "    simple_CBOW 에서의 방식 : 100만 개의 단어 중 옳은 단어 하나를 선택하는 문제   \n",
    "\n",
    "        you, goodbye ---> 타깃단어는 무엇인가? ---> say (1/100만)\n",
    "\n",
    "    네거티브 샘플링 방식 : 100만 개의 단어 중 특정 단어가 옳은 단어인지 아닌지 이진 판단하는 문제\n",
    "\n",
    "        you, goodbye ---> say인가? (Yes, No)\n",
    "\n",
    "본 방법을 구현하기 위해서는 ?\n",
    "#### > 출력층에 뉴런을 하나만 준비하면 된다. 출력층의 뉴런이 `say`의 점수를 출력하는 것이다. \n",
    "\n",
    "`이전에는 출력층에 softmax로 모든 단어에 대한 확률을 출력한 뒤, 가장 높은 확률의 단어를 예측하는 식으로 계산을 수행했다면,    \n",
    "이제는 출력층의 say에 해당하는 단어에 sigmoid 함수를 적용시켜 say에 해당하는 확률값만을 받아온다.\n",
    "모든 단어에 대한 계산을 단일 단어에 대한 계산으로 변환`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시그모이드 함수와 교차 엔트로피 오차\n",
    "\n",
    "sigmoid 함수 : \n",
    "\n",
    "$ y = \\frac{1}{1 + exp(-x)} $\n",
    "\n",
    "교차 엔트로피 오차 : \n",
    "\n",
    "$L = -(tlogy + (1-t)log(1-y))$\n",
    "\n",
    " - y : 시그모이드 함수의 출력\n",
    " - t : 정답 레이블 (1 혹은 0)\n",
    " - t = 1  > 정답 > -logy\n",
    " - t = 0 > 정답 아님 > -log(1-y)\n",
    " \n",
    "**Sigmoid + Cross Entropy Error를 조합하여 역전파의 값이 y-t 라는 값으로 깔끔하게 도출된다!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 분류에서 이진 분류로 (구현)\n",
    "\n",
    "Embed -> hidden -> Embed dot -> Sigmoid with loss\n",
    "\n",
    "** Embedding Dot Layer : Embedding Layer + dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        # 총 4개의 인스턴스 변수\n",
    "        self.embed = Embedding(W) # Embedding 계층\n",
    "        self.params = self.embed.params # 매개변수 저장\n",
    "        self.grads = self.embed.grads # 기울기 저장\n",
    "        self.cache = None # 순전파 시의 계산 결과를 잠시 유지하기 위해 사용되는 변수 \n",
    "        \n",
    "    # 순전파 메서드에서는 은닉층 뉴런과 단어 ID의 넘파이 배열(미니배치)을 받는다.\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx) # embedding 계층의 forward(idx)를 호출하여 idx에 해당하는 행 추출\n",
    "        out = np.sum(target_W * h, axis = 1) # 내적 계산 이후 행마다 더하여 최종결과 out 반환\n",
    "        \n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0],1)\n",
    "        \n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 3,  4,  5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [0,3,1]\n",
    "target_W = W[idx]\n",
    "target_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.arange(9).reshape(3,3)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  4],\n",
       "       [27, 40, 55],\n",
       "       [18, 28, 40]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_W * h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5, 122,  86])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(target_W * h, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네거티브 샘플링\n",
    "\n",
    "현재까지는 정답에 대해서만 학습.    \n",
    "예를 들어, `you`와 `goodbye` 가 맥락 입력으로 들어갔을 때 정답 레이블이 `say`인 경우, 좋은 가중치가 준비되어 있다면 Sigmoid 계층의 확률은 1에 가까울 것이다\n",
    "\n",
    "**But 오답 (say 이외의 단어)을 입력하면 어떤 결과가 나올지에 대해서도 처리해줘야함!**\n",
    "\n",
    "우리의 목적 :\n",
    "1. 긍정적 예 (\"say\")에 대해서는 Sigmoid 의 출력이 1에 가깝게 \n",
    "2. 부정적 예 (\"say\" 외 단어)에 대해서는 Sigmoid의 출력이 0에 가깝게\n",
    "\n",
    "`어떻게 2번을 구현할 수 있을까?`\n",
    "\n",
    "모든 부정적 예를 대상으로 이진 분류 학습 ? \n",
    "**No** 이전과 마찬가지로 high cost!\n",
    "\n",
    "**근사적인 해답으로 부정적 예를 몇 개 선택한다. 즉, 적은 수의 부정적 예를 샘플링해 사용한다. (Negative Sampling)**\n",
    "\n",
    "---\n",
    "### Summary\n",
    "1. 네거티브 샘플링 기법은 긍정적 예를 타깃으로 한 경우의 손실을 구한다. \n",
    "2. 동시에 부정적 예를 몇 개 샘플링(선별)하여 각 부정적 예에 대하여 손실을 구한다. \n",
    "3. (1 + 2) 각 손실을 더한 값을 최종 손실로 정한다.\n",
    "\n",
    "### Example\n",
    "\n",
    "1. say ... 1\n",
    "2. hello ... 0 K ... 0\n",
    "3. sum(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네거티브 샘플링의 샘플링 기법\n",
    "\n",
    "**그렇다면 부정적 예를 어떻게 샘플링 하는가?**\n",
    "\n",
    "무작위 샘플링? \n",
    "\n",
    "    No! 희소한 단어만 샘플링되었다면 결과가 나빠질 것\n",
    "\n",
    "말뭉치의 통계 데이터를 기초로 샘플링해보자. \n",
    "\n",
    "**말뭉치에서 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출하자**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 0-9 숫자 중 하나 무작위 샘플링\n",
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어에서도 마찬가지\n",
    "words = ['you', 'say', 'goodbye' ,'I', 'hello', '.']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.' 'hello' 'hello' 'say' 'goodbye']\n",
      "['goodbye' 'you' 'hello' '.' 'say']\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "# 중복 허용 추출\n",
    "print(np.random.choice(words, size = 5))\n",
    "# 5개만 무작위 샘플링 (중복 없음)\n",
    "print(np.random.choice(words, size = 5, replace = False))\n",
    "# 확률분포에 따라 샘플링\n",
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "print(np.random.choice(words, p=p)) # 인수 p에 확률분포 리스트를 지정하면 확률분포대로 샘플링한다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편, word2vec의 네거티브 샘플링에서는 앞의 확률분포 (p)에 0.75를 제곱하라고 권고한다.\n",
    "\n",
    "**why?**  <u>0.75 제곱을 함으로써 출현 확률이 낮은 단어를 버리지 않기 위해</u>\n",
    "\n",
    "**0.75 라는 수치에 대한 이론적 근거는 없다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64196878 0.33150408 0.02652714]\n"
     ]
    }
   ],
   "source": [
    "p = [0.7, 0.29, 0.01]\n",
    "new_p = np.power(p, 0.75)\n",
    "new_p /= np.sum(new_p)\n",
    "print(new_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.75 제곱을 통해 낮은 확률이 약간 상승함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramSampler:\n",
    "    # 초기화 시에 3개의 인수를 받는다\n",
    "    # 단어 ID 목록, 확률분포에 제곱할 값, 부정적 예시 샘플링할 개수\n",
    "    def __init__(self, corpus, power, sample_size): \n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        # 단어 빈도 산출\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "        \n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "        \n",
    "        # 단어의 빈도 기준 확률 분포 산출\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "        \n",
    "    \n",
    "    # target으로 지정한 단어를 긍정적 예로 해석하고, 그 외의 단어 ID를 샘플링\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "        GPU = False\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # GPU(cupy）로 계산할 때는 속도를 우선한다.\n",
    "            # 부정적 예에 타깃이 포함될 수 있다.\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [0 1]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1,3,0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. target 1에 해당하는 부정적 예시\n",
    "2. target 3에 해당하는 부정적 예시 샘플링\n",
    "3. target 0에 해당하는 부정적 예시 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네거티브 샘플링 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    # 출력 가중치 W, 말뭉치 ID 리스트, 확률분포에 제곱할 값, 샘플링 횟수\n",
    "    def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "\n",
    "        # 원하는 계층을 리스트로 보관\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)] # 부정적 예시(sample_size) + 긍정적 예시 (1)\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        \n",
    "        self.params, self.grads = [],[] # 이 계층에서 사용하는 매개변수와 기울기를 각각 배열로 저장\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target) # 부정적 예를 샘플링하여 변수에 저장\n",
    "        \n",
    "        #긍정적 예 순전파. 0번째 계층\n",
    "        score = self.embed_dot_layers[0].forward(h,target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32) # 1\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        #부정적 예 순전파 \n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32) # 0\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:,i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target) \n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "            \n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개선판 word2vec 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    # 어휘수, 은닉층 뉴런수, 맥락 크기, 단어 ID 목록\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f') # embedding layer를 사용하므로 둘의 형상이 같다.\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embedding 계층 2*window_size만큼 사용\n",
    "            self.in_layers.append(layer) # 배열로 보관\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target) # loss가 다르다! 간결해짐\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ===============================================\n",
    "#config.GPU = True\n",
    "# ===============================================\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from cbow import CBOW\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 9295 | time 0[s] | loss 4.16\n",
      "| epoch 1 |  iter 21 / 9295 | time 1[s] | loss 4.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a8af64f773d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 학습 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google 드라이브/2020/Study/[2019.12-] Keracorn/Keracorn-NLP-Study/common/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# 勾配を求め、パラメータを更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 共有された重みを1つに集約\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google 드라이브/2020/Study/[2019.12-] Keracorn/Keracorn-NLP-Study/ch04/cbow.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, contexts, target)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss가 다르다! 간결해짐\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google 드라이브/2020/Study/[2019.12-] Keracorn/Keracorn-NLP-Study/ch04/negative_sampling_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, target)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mnegative_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_negative_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 부정적 예를 샘플링하여 변수에 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# 긍정적 예 순전파. 0번째 계층\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google 드라이브/2020/Study/[2019.12-] Keracorn/Keracorn-NLP-Study/ch04/negative_sampling_layer.py\u001b[0m in \u001b[0;36mget_negative_sample\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mnegative_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# GPU(cupy）로 계산할 때는 속도를 우선한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정 - 사용하는 말뭉치의 수에 따라 다르게 설정\n",
    "# 윈도우 크기는 2 - 10 이 good\n",
    "window_size = 5  \n",
    "# 단어의 분산 표현의 차원수 - 50-500 good\n",
    "hidden_size = 100 \n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs # W_in 꺼내 사용\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "\n",
    "# 이런 식으로 저장을 하는구만\n",
    "params = {} \n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1) # 피클로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델 평가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.util import most_similar #2장에서 구현\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 매개변수 불러오기\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    \n",
    "word_vecs = params['word_vecs']\n",
    "word_to_id = params['word_to_id']\n",
    "id_to_word = params['id_to_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(word_vecs.shape)\n",
    "print(len(word_to_id))\n",
    "print(len(id_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1000개의 단어 벡터 저장됨. \n",
    "- 각 단어 벡터 당 100차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query]you\n",
      " we: 0.6103515625\n",
      " someone: 0.59130859375\n",
      " i: 0.55419921875\n",
      " something: 0.48974609375\n",
      " anyone: 0.47314453125\n",
      "\n",
      "[query]year\n",
      " month: 0.71875\n",
      " week: 0.65234375\n",
      " spring: 0.62744140625\n",
      " summer: 0.6259765625\n",
      " decade: 0.603515625\n",
      "\n",
      "[query]car\n",
      " luxury: 0.497314453125\n",
      " arabia: 0.47802734375\n",
      " auto: 0.47119140625\n",
      " disk-drive: 0.450927734375\n",
      " travel: 0.4091796875\n",
      "\n",
      "[query]toyota\n",
      " ford: 0.55078125\n",
      " instrumentation: 0.509765625\n",
      " mazda: 0.49365234375\n",
      " bethlehem: 0.47509765625\n",
      " nissan: 0.474853515625\n"
     ]
    }
   ],
   "source": [
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "\n",
    "for q in querys:\n",
    "    most_similar(q, word_to_id, id_to_word, word_vecs, top = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "괜찮은 결과가 나왔다 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec으로 얻은 단어의 분산 표현이 할 수 있는 것\n",
    "- 비슷한 단어를 가까이 모은다 \n",
    "- 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있다 ; \"king - man + woman = queen\"\n",
    "- **man -> woman** 벡터와 **king -> ?** 벡터가 가까워지는 단어를 찾는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    ''' 배열 x 의 element 들의 value를 -1 ~ 1 사이로 정규화\n",
    "    '''\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1)) \n",
    "        x = x.astype(np.float32) # 형 변환 처리해야 에러 안 발생\n",
    "        x /= s.reshape(s.shape[0],1)\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x = x.astype(np.float32)\n",
    "        x /= s\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0114111 , -0.13201074, -0.00794302, -0.00223747,  0.05854714,\n",
       "        0.03268571,  0.01316378, -0.07327715,  0.04538335,  0.02606653,\n",
       "        0.06779534, -0.01249254, -0.00656325, -0.00794302, -0.07286695,\n",
       "       -0.15177506,  0.00082041, -0.1330549 , -0.05806235,  0.03773866,\n",
       "        0.14461516,  0.12082339, -0.05369928, -0.00193914,  0.01297733,\n",
       "       -0.2585024 ,  0.11306682, -0.13603818, -0.01133652,  0.0903938 ,\n",
       "        0.11858592, -0.11754177,  0.03367393, -0.11836217, -0.10031325,\n",
       "        0.11455847, -0.0663037 , -0.02017452,  0.03024314, -0.04743437,\n",
       "       -0.1875    , -0.00626492,  0.09307876, -0.06511039, -0.12820704,\n",
       "       -0.01159755, -0.200179  , -0.04694958, -0.11426014, -0.1605012 ,\n",
       "       -0.17213604, -0.03773866,  0.20629475, -0.04116945,  0.05299075,\n",
       "       -0.01715394,  0.03367393, -0.03400955, -0.09471957, -0.04527148,\n",
       "        0.03262977, -0.02369854,  0.0650358 , -0.075179  ,  0.02367989,\n",
       "       -0.03235009, -0.00152894, -0.00693616,  0.33383054,  0.08860382,\n",
       "        0.25059667, -0.10777148,  0.17884845, -0.174821  ,  0.05086515,\n",
       "       -0.03147375, -0.00395286,  0.20704058,  0.0629475 , -0.03710471,\n",
       "        0.06317124,  0.10381862, -0.046875  ,  0.03818616, -0.08368138,\n",
       "       -0.03169749,  0.0620898 , -0.07890812,  0.02425791,  0.04493586,\n",
       "        0.11754177,  0.06768347,  0.11701969,  0.13670942,  0.01118735,\n",
       "        0.02953461,  0.12261336, -0.02860233,  0.11590096,  0.12440334],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(word_vecs[word_to_id['king']]-word_vecs[word_to_id['man']]+word_vecs[word_to_id['woman']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    ''' a : c = b : ? 유추 문제 풀기\n",
    "    e.g., man : woman = king : ? ==> woman\n",
    "    '''\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print(\"%s(을)를 찾을 수 없습니다.\" % word)\n",
    "            return\n",
    "    print(\"\\n[analogy]\" + a + \":\" + b + '=' + c + \":?\")\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec) \n",
    "    \n",
    "    #### 가장 유사한 벡터를 dot product 연산을 통해 구함\n",
    "    #### 유사할수록 score가 높게 나올 것이다! 유사하니까.. 정규화된 값과 곱할 경우 값이 1에 가까움\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "    \n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i],similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy]man:king=woman:?\n",
      " she: 4.178053855895996\n",
      " moody: 4.131805419921875\n",
      " share: 4.0512566566467285\n",
      " character: 3.967434883117676\n",
      " chain: 3.9129433631896973\n"
     ]
    }
   ],
   "source": [
    "analogy('man','king','woman',word_to_id, id_to_word, word_vecs,top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy]take:took=go:?\n",
      " went: 4.549774169921875\n",
      " points: 4.250325679779053\n",
      " began: 4.091780662536621\n",
      " comes: 3.981295108795166\n",
      " oct.: 3.9054713249206543\n"
     ]
    }
   ],
   "source": [
    "analogy('take','took','go',word_to_id, id_to_word, word_vecs,top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy]car:cars=child:?\n",
      " children: 5.218576908111572\n",
      " average: 4.726343631744385\n",
      " yield: 4.207968711853027\n",
      " cattle: 4.187469482421875\n",
      " priced: 4.17901611328125\n"
     ]
    }
   ],
   "source": [
    "analogy('car','cars','child',word_to_id, id_to_word, word_vecs,top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy]good:better=bad:?\n",
      " more: 6.6468095779418945\n",
      " less: 6.063014030456543\n",
      " rather: 5.220152378082275\n",
      " slower: 4.7328972816467285\n",
      " greater: 4.672525405883789\n"
     ]
    }
   ],
   "source": [
    "analogy('good','better','bad',word_to_id, id_to_word, word_vecs,top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재형 과거형 시제 파악\n",
    "- 단수형 복수형 파악\n",
    "\n",
    "**==> 문법적인 패턴까지 파악가능한 분산 표현!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec 남은 주제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec을 사용한 어플리케이션의 예\n",
    "\n",
    "단어의 분산 표현의 강점! \n",
    "\n",
    "**1. 전이 학습을 가능하게 한다.**   \n",
    "\n",
    "사전에 큰 말뭉치로 학습을 끝낸 후, 그 분산 표현을 이후 작업에 이용\n",
    "- 텍스트 분류 \n",
    "- 문서 클러스터링\n",
    "- 품사 태깅\n",
    "- 감정 분석\n",
    "\n",
    "자연어 처리 태스크에서 학습을 미리 끝낸 단어의 분산 표현을 이용하면 효과적이다!\n",
    "\n",
    "**2. 단어를 고정 길이 벡터로 변환해준다.**\n",
    "\n",
    "문장 또한 단어의 분산 표현을 활용하여 고정 길이 벡터로 변환할 수 있다\n",
    "\n",
    "가장 간단한 방법인 `bag-of-words`는 단어의 순서를 고려하지 않는 모델   \n",
    "\n",
    "    : 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구함\n",
    "    \n",
    "단어를 고정 길이 벡터로 변환할 수 있다는 점은 매우 중요하다. 고정 길이의 벡터로 변환할 수 있다면 일반적인 머신러닝 기법이 적용 가능해진다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 벡터 평가 방법\n",
    "\n",
    "단어의 분산 표현이 단어를 얼마나 정확하게 표현해내는가?\n",
    "\n",
    "--> 단어의 `유사성`과 단어의 `유추 문제`를 활용한 평가\n",
    "\n",
    "1. 유사성 평가 \n",
    "\n",
    "유사도를 0에서 10 사이로 점수화할 때 'cat'과 'animal'의 유사도는 8점. 'cat'과 'car'의 유사도는 2점과 같이, **사람**이 단어 사이의 유사한 정도를 규정한다. \n",
    "\n",
    "\n",
    "2. 유추 문제 활용한 평가\n",
    "\n",
    "\"king : queen = man : ?\" 과 같은 유추 문제를 출제하고 그 **정답률**로 단어의 분산 표현의 우수성을 측정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/fig 4-23.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델에 따라 정확도가 다르다. 대체로 skip-gram 모델이 CBOW 모델보다 정답률이 높다\n",
    "- 일반적으로 말뭉치가 클수록 결과가 좋다. 데이터는 항상 다다익선\n",
    "- 단어 벡터 차원 수는 적당한 크기가 좋다. 너무 커도 결과가 나빠진다. 300 차원이 1000차원보다 정답률이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word analogy test\n",
    "\n",
    "    Syntactic:\n",
    "    e.g., bad : worst = good : best\n",
    "    Semantics:\n",
    "    e.g., Seoul : Korea = Tokyo : Japan\n",
    "\n",
    "**Why does this imply that LMs are well trained? What does it have to do with vector arithmetic? When does this hold?**\n",
    "\n",
    "--> word2vec을 수학적으로 접근해서 성능을 분석하는 논문이 2019년 ACL에 발표되었다.\n",
    "\n",
    "#### Ethayarajh et al., Towards Understanding Linear Word Analogies, ACL 2019\n",
    "\n",
    "word analogy 를 함수로 접근!\n",
    "\n",
    "Analogy 𝑓 가 set of ordered pairs 𝑆 상의 invertible transformation이다\n",
    "\n",
    "\n",
    "seoul, Korea -> 한국 `수도` = 서울 / function = \"의 수도\"!\n",
    "\n",
    "https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html\n",
    "\n",
    "<img src=\"../imgs/word_analogy.png\" width=\"1000\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
