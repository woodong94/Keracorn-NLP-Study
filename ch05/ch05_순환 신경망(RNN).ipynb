{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 살펴본 신경망은 피드포워드 유형의 신경망이다. 피드포워드란 흐름이 단방향을 신경망을 말한다.\n",
    "\n",
    "그러나 피드포워드 신경망은 **시계열 데이터의 성질(패턴)을 충분히 학습할 수 없다**는 커다란 단점이 있다.\n",
    "\n",
    "그래서 **순환 신경망 <sup>Recurrent Neural Network</sup> (RNN)** 이 등장하게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률과 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec을 확률 관점에서 바라보다\n",
    "\n",
    "먼저 word2vec의 CBOW 모델을 간단히 복습해보자. \n",
    "\n",
    "말뭉치 $w_1, w_2, ...,w_{t-1},w_{t},w_{t+1},,..., w_T$ 에서 $w_{t-1}$과 $w_{t+1}$이 주어졌을 때 타깃 $w_t$가 될 확률은 다음과 같다. \n",
    "\n",
    "$$P(w_t|w_{t-1},w_{t+1})$$\n",
    "\n",
    "이번에는 맥락을 왼쪽 윈도우만으로 한정해보자.\n",
    "\n",
    "<img src=\"../imgs/fig 5-2.png\" width=\"400\" align='center'>\n",
    "\n",
    "이제 확률 식은 다음으로 같다.\n",
    "\n",
    "$$P(w_t|w_{t-2},w_{t-1})$$\n",
    "\n",
    "이전에 나온 단어들의 확률 값으로 이후 단어를 예측하는 것. 이 식으로부터 언어 모델이 등장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어 모델\n",
    "\n",
    "언어 모델 <sup>Language Model</sup>은 **특정한 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도인지 (얼마나 자연스러운 단어 순서인지)를 확률로 평가한다.**\n",
    "\n",
    "    - \"you say goodbye\" : 높은 확률\n",
    "    - \"you say good die\" : 낮은 확률\n",
    "\n",
    "응용 분야 \n",
    "\n",
    "- 기계 번역과 음성 인식에서 문장이 얼마나 자연스러운지 판단하여 더 높은 자연스러움을 가진 문장을 반환\n",
    "- 단어 순서의 자연스러움을 토대로 새로운 문장을 생성\n",
    "\n",
    "---\n",
    "\n",
    "#### 이제 언어 모델을 수식으로 이해해보자\n",
    "\n",
    "$w_1,...,w_m$ 이라는 m개의 단어로 된 문장을 생각해보자.\n",
    "\n",
    "이때 단어가 $w_1,...,w_m$이라는 순서로 출현할 확률을 $P(w_1,...,w_m)$의 동시 확률로 나타낼 수 있다.\n",
    "\n",
    "이 동시 확률은 다음과 같이 분해하여 쓸 수 있다. \n",
    "\n",
    "<img src=\"../imgs/e 5-4.png\" width=\"400\" align='center'>\n",
    "\n",
    "동시 확률 $P(w_1,...,w_m)$는 사후 확률의 총 곱인 $\\prod{P(w_t|w_1,...,w_{t-1})}$ 으로 대표될 수 있다.    \n",
    "여기서의 사후 확률은 **타깃 단어보다 왼쪽에 있는 모든 단어**를 맥락으로 했을 때의 확률과 같다. \n",
    "\n",
    "즉, 단어가 순서대로 출현할 확률은 동시 확률로 나타내질 수 있고, 이 동시 확률은 사후 확률의 총 곱으로 나타내질 수 있으니까. 우리의 목표는 사후 확률 $P(w_t|w_1,...,w_{t-1})$를 구하는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델을 언어 모델로?\n",
    "\n",
    "그렇다면 word2vec의 CBOW 모델을 억지로 언어 모델에 적용하려면??\n",
    "\n",
    "-> 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타낼 수 있다! 맥락의 크기를 왼쪽 2개 단어로 한정한다면, 다음과 같이 표현 가능\n",
    "\n",
    "$P(w_1,...,w_m) = \\prod_{t=1}^{m}{P(w_t|w_1,...,w_{t-1})}\\approx \\prod_{t=1}^{m}{P(w_t|w_{t-2},w_{t-1})}$\n",
    "\n",
    "맥락의 길이는 5나 10으로 임의로 설정 가능. 어쨌거나 **맥락의 크기는 특정 크기로 고정**된다.\n",
    "\n",
    "---\n",
    "\n",
    "### 한계\n",
    "\n",
    "    이처럼 맥락의 크기가 고정될 경우 아래와 같이 맥락 크기보다 앞에 단서 단어가 나오는 케이스는 해결하기 힘들다.\n",
    "\n",
    " `Tom` was watching TV in his room. Mary came into the room. Mary said hi to `?`\n",
    "    \n",
    "   **1) 그렇다면 맥락의 크기를 20이나 30으로 키우면 되지 않을까?**\n",
    "\n",
    "    but CBOW 모델에서는 레이어 내부에서 단어 벡터들의 합계를 내는 과정에서 맥락 안의 순서가 무시되기 때문에 적합하지 않다.\n",
    "\n",
    "   **2) 엥 그렇다면 단어 벡터들 합하지 말고 concatenate하면 되지 않을까??**\n",
    "    \n",
    "    맥락의 크기에 비례해 매개변수가 증가한다. \n",
    "\n",
    "그러나 RNN의 단점도 많은 것으로 아는데..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### `세상에 두달만에 다시 시작;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN이란\n",
    "\n",
    "RNN <sup>Recurrent Neural Network</sup>의 `Recurrent` : `몇 번이나 반복해서 일어나는 일, 순환한다`\n",
    "\n",
    "즉, RNN은 우리 말로 **순환 신경망**이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순환 신경망\n",
    "\n",
    "`순환`하기 위해서는 닫힌 경로 혹은 순환하는 경로가 존재해야 한다. 그래야 데이터가 순환하면서 정보가 끊임없이 갱신된다. like loop!\n",
    "\n",
    "<img src=\"../imgs/fig 5-7.png\" width=\"400\" align='center'>\n",
    "\n",
    "- 그림의 RNN계층은 $X_t$를 입력받는데, 이때 t는 시각을 뜻한다.\n",
    "- 시계열 데이터 $(x_0, x_1, x_2, x_3, ...) $ 를 의미함\n",
    "- 입력에 대응하여 $(h_0, h_1, h_2, h_3, ...) $ 가 출력된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순환 구조 펼치기\n",
    "\n",
    "<img src=\"../imgs/fig 5-8.png\" width=\"700\" align='center'>\n",
    "\n",
    "#### vs 피드포워드 신경망   \n",
    "**같은 점**\n",
    " - RNN 계층의 순환 구조를 펼침으로써 오른쪽으로 성장하는 긴 신경망으로 변신시킬 수 있다! Like 피드포워드 신경망!   \n",
    " \n",
    "**다른 점**\n",
    "- BUT 다수의 RNN 계층 모두가 실제로는 같은 계층인 것이 다르다!\n",
    "\n",
    "---\n",
    "**각 시각의 RNN 계층은 그 계층으로의 입력과 바로 직전의 RNN 계층으로부터의 출력을 받는다. 그리고 이 두 정보를 바탕으로 현 시각의 출력을 계산한다.**\n",
    "\n",
    " $$h_t = tanh(h_{t-1}W_h + x_tW_x + b) ....[식 5.9]$$\n",
    "\n",
    "- RNN에는 가중치가 2개 있다\n",
    "    - 입력 x를 출력 h로 변환하기 위한 가중치 $W_x$ - 화살표 위\n",
    "    - 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치 $W_h$ - 분기 화살표\n",
    "    \n",
    "\n",
    "- 식 5.9에서는 행렬 곱을 계산하고 그 합을 tanh 함수를 이용해 변환한다. 그 결과가 시각 t의 출력 $h_t$\n",
    "- **$h_t$는 다른 계층을 향해 위쪽으로 출력되는 동시에, 다음 시각의 RNN 계층(자기 자신)을 향해 오른쪽으로도 출력됨!**\n",
    "\n",
    "#### 현재의 출력 $h_t$은 한 시각 이전 출력 $h_{t-1}$에 기초해 계산된다!\n",
    "\n",
    "그래서 RNN 계층을 `메모리가 있는 계층` 이라고도 말한다.\n",
    "\n",
    "<sup>** $h_t$ : hidden state</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT \n",
    "\n",
    "BPTT<sup>Backpropagation Through Time</sup> ==> 시간 방향으로 펼친 신경망의 오차역전파법\n",
    "\n",
    "**BUT 긴-- 시계열 데이터 학습하기 어렵다!**\n",
    "- 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원이 증가함. 메모리 DEAD ㅠㅠ\n",
    "- 기울기도 불안정해짐. Gradient Descent Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated BPTT\n",
    "\n",
    "**큰 시계열 데이터를 취급할 때, 신경망 연결을 적당한 길이로 끊어 작은 신경망 여러 개로 만든다는 아이디어**\n",
    "\n",
    "Truncated BPTT ==> 적당한 길이로 잘라낸 오차역전파법!\n",
    "\n",
    "#### **** 주의 ****\n",
    "\n",
    "- 순전파의 연결은 유지한다\n",
    "- 역전파의 연결만을 끊어낸다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "역전파의 연결만을 끊어낸다는 것이 무슨의미일까 \n",
    "\n",
    "<img src=\"../imgs/fig 5-11.png\" width=\"800\" align = \"center\">\n",
    "\n",
    "- RNN 계층을 길이 10개 단위로 학습할 수 있도록 역전파 연결을 끊음\n",
    "- 그보다 미래의 데이터에 대해서는 생각할 필요가 없다! **각각의 블록 단위로, 미래의 블록과는 독립적으로 오차역전파법을 완결시킴**\n",
    "\n",
    "\n",
    "<img src=\"../imgs/fig 5-14.png\" width=\"800\" align = \"center\">\n",
    "\n",
    "\n",
    "- Step 1\n",
    "\n",
    "    1) 순전파) 입력데이터 x0 - x9 (10블록) ==> h0 - h9 출력   \n",
    "    2) 역전파) dh9--->dx9 - dh0--->dx0\n",
    "    \n",
    "- Step 2\n",
    "\n",
    "    1) 순전파) Step 1의 마지막 은닉 상태인 h9를 통해 계층 연결! 입력데이터 x10 - x19 (10블록) ==> h0 - h9 출력   \n",
    "    2) 역전파) dh19--->dx19 - dh10--->dx10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated BPTT의 미니배치 학습\n",
    "\n",
    "길이가 1,000인 시계열 데이터에 대해서 시각의 길이를 10개 단위로 잘라 Truncated BPTT로 학습하는 경우!\n",
    "\n",
    "- 첫 번째 미니배치 때는 처음부터 순서대로 데이터를 제공 \n",
    "- 두 번째 미니배치 때는 500번째의 데이터를 시작위치로 정하고, 그 위치부터 다시 순서대로 데이터를 제공\n",
    "\n",
    "<img src=\"../imgs/fig 5-15.png\" width=\"600\" align = \"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/fig 5-17.png\" width=\"600\" align = \"center\">\n",
    "\n",
    "- `RNN 계층` : Time RNN 계층 내에서 한 단계의 작업을 수행하는 계층\n",
    "- `Time RNN 계층` : T개 단계 분의 작업을 한꺼번에 처리하는 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 계층 구현\n",
    "\n",
    " $$h_t = tanh(h_{t-1}W_h + x_tW_x + b) ....[식 5.9]$$\n",
    " \n",
    "행렬 계산 시에는 형상이 중요하다!\n",
    "- 미니배치 크기가 N, 입력 벡터의 차원 수가 D, 은닉 상태 벡터의 차원 수가 H\n",
    "\n",
    "<img src=\"../imgs/fig 5-18.png\" width=\"400\" align = \"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b): # 가중치 2개와 편향 1개 인수로\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)] #numpy.zeros_like : shape 유지하고 0으로 초기화\n",
    "        self.cache = None # *** 역전파 계산 시 사용하는 중간 데이터 담는 곳\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b # Main 식\n",
    "        h_next = np.tanh(t) # 다음 시각 계층으로의 입력\n",
    "        \n",
    "        self.cache = (x,h_prev,h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - h_next**2) # tanh 미분 \n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.matmul(h_prev.T, dt)\n",
    "        dh_prev = np.matmul(dt, Wh.T)\n",
    "        dWx = np.matmul(x.T, dt)\n",
    "        dx = np.matmul(dt, Wx.T)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx,dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/fig 5-19.png\" width=\"450\" align = \"left\">\n",
    "<img src=\"../imgs/fig 5-20.png\" width=\"400\" align = \"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time RNN 계층 구현\n",
    "\n",
    "**Time RNN 계층은 T개의 RNN 계층으로 구성된다**\n",
    "\n",
    "- RNN 계층의 은닉 상태 h를 인스턴스 변수로 유지한다. 이 변수를 다음 RNN 레이어에 인계해주는 용도로 이용한다.\n",
    "\n",
    "<img src= \"../imgs/fig 5-22.png\" width=\"700\" align = \"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False): #stateful : 은닉상태 인계 받을지 여부\n",
    "        self.params = [Wx, Wh, b ]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None # T 개의 RNN 계층 리스트로 저장하는 용도\n",
    "        \n",
    "        # h : forward() 메서드 이후 마지막 RNN 계층의 은닉상태 저장\n",
    "        # dh : backward() 메서드 이후 하나의 앞 블록의 은닉 상태의 기울기 저장\n",
    "        self.h, self.dh = None, None \n",
    "        # True : 아무리 긴 시계열 데이터여도 순전파를 끊지 않고 전파\n",
    "        # False : 은닉 상태를 영행렬 (모든 요소가 0 행렬)로 초기화 \n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def set_state(self,h):# 은닉상태 설정 \n",
    "        self.h = h\n",
    "        \n",
    "    def reset_state(self): # 은닉상태 초기화\n",
    "        self.h = None\n",
    "        \n",
    "    # 순전파에서 입력 xs를 받는다\n",
    "    # xs : T 개 분량의 시계열 데이터를 하나로 모은 것\n",
    "    def forward(self, xs): \n",
    "        Wx, Wh, b = self.params\n",
    "        # 미니배치크기 N, 시계열 데이터 T개, 입력 벡터 차원수 D\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        # 출력값 담을 그릇\n",
    "        hs = np.empty((N, T, H), dtype = 'f') \n",
    "        \n",
    "        # \"stateful이 false\" 이거나 \"처음 호출 \" 일때 영행렬로 초기화\n",
    "        if not self.stateful or self.h is None: \n",
    "            self.h = np.zeros((N,H),dtype='f')\n",
    "            \n",
    "        # RNN 계층이 각 시간 t의 은닉 상태 h를 계산하고 이를 hs에 저장   \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params) \n",
    "            self.h = layer.forward(xs[:,t,:], self.h) \n",
    "            hs[:,t,:] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # forward가 처음 호출되면 h에는 마지막 RNN 계층의 은닉 상태가 저장됨\n",
    "        # 다음번 forward 호출 시 stateful이 True면 먼저 저장된 h 값이 그대로 이용되고 False면 영행렬로 초기화\n",
    "        return hs\n",
    "    \n",
    "    # 역전파\n",
    "    def backward(self,dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        dxs = np.empty((N,T,D),dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0,0,0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            # RNN 계층의 순전파에서는 출력이 2개로 분기되어 역전파에서 각 기울기가 합산되어 전해짐\n",
    "            dx, dh = layer.backward(dhs[:,t,:] + dh) # --> 합산된 기울기\n",
    "            dxs[:,t,:] = dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "                \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"../imgs/fig 5-24.png\" width=\"600\" align = \"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
