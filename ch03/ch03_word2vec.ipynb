{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T19:20:38.365049Z",
     "start_time": "2019-12-27T19:20:38.361839Z"
    }
   },
   "source": [
    "# 추론 기반 기법과 신경망\n",
    "\n",
    "\n",
    "## 통계 기반 기법의 문제점\n",
    "\n",
    "통계 기반 기법의 문제점은 대규모 말뭉치를 다룰 때 $O(n^3)$의 비용이 든다는 점\n",
    "\n",
    "- `통계 기반 기법` : 학습 데이터를 한꺼번에 처리 (배치 학습)\n",
    "- `추론 기반 기법`: 학습 데이터의 일부를 사용하여 순차적으로 학습(미니배치 학습)\n",
    "\n",
    "미니배치 학습은 계산량이 큰 작업을 처리할 때 효율적. 데이터를 작게 나눠 학습하기 때문에 연산 가능. 또한 병렬 계산도 가능하게 하여 학습 속도 또한 높일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T19:21:48.195984Z",
     "start_time": "2019-12-27T19:21:48.187181Z"
    }
   },
   "source": [
    "## 추론 기반 기법 개요\n",
    "```\n",
    "you ? goodbye and I say hello.\n",
    "```\n",
    "문장에서 ?을 추론하는 작업. 모델은 이러한 추론 문제를 반복해서 풀면서 **단어의 출현 패턴을 학습**한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망에서의 단어 처리\n",
    "\n",
    "신경망을 이용해 단어를 처리하기 위해서는 단어를 **고정 길이의 벡터**로 변환해야 한다.\n",
    "\n",
    "대표적인 방법이 `원핫 표현 (one-hot encoding)`\n",
    "\n",
    "입력층의 각 뉴런이 각 단어에 대응됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:56.072982Z",
     "start_time": "2020-01-06T06:25:55.844007Z"
    }
   },
   "outputs": [],
   "source": [
    "# 완전연결계층에 의한 원핫 인코딩 형식의 단어 변환\n",
    "# 7 X 3 계층, bias는 생략됨\n",
    "import numpy as np\n",
    "\n",
    "c = np.array([1, 0, 0, 0, 0, 0, 0]) # 원 핫 입력\n",
    "W = np.random.randn(7,3) # 가중치\n",
    "h = np.matmul(c,W) # 중간 노드\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:24:10.482331Z",
     "start_time": "2020-01-02T06:24:10.469106Z"
    }
   },
   "source": [
    "원핫 표현의 단어 ID에 대응하는 원소만 1이고 그 외에는 0인 벡터.\n",
    "\n",
    "즉, c와 W의 행렬 곱은 **가중치의 행벡터 하나**를 뽑아낸 것과 같다.\n",
    "\n",
    "때문에 비효율적인 계산 방식. (4장에서 개선)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단순한 word2vec\n",
    "\n",
    "## CBOW 모델의 추론 처리\n",
    "\n",
    "CBOW <sup>continuous bag-of-words<sup>\n",
    "    \n",
    "CBOW 모델은 **맥락으로부터 타깃(target)을 추측**하는 용도의 신경망이다. \n",
    "\n",
    "- 타깃 : 중앙 단어\n",
    "- 맥락 : 중앙 단어의 주변 단어들\n",
    "\n",
    "<u>맥락이 입력이고 타깃이 출력이다</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:35:54.099273Z",
     "start_time": "2020-01-02T06:35:54.087577Z"
    }
   },
   "source": [
    "<img src=\"../imgs/CBOW.png\" width=\"300\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:39:05.797096Z",
     "start_time": "2020-01-02T06:39:05.792535Z"
    }
   },
   "source": [
    "위 그림을 보면 입력층이 4개 있고, 은닉층을 거쳐 출력층에 도달한다. \n",
    "\n",
    "4개의 입력층에서 은닉층으로의 변환은 완전연결계층 $W_{in}$ 이 처리한다. \n",
    "\n",
    "그리고 은닉층에서 출력층 뉴런으로의 변환은 다른 완전연결계층 $W_{out}$이 처리한다. \n",
    "\n",
    "위 그림에서 입력층이 4개인 이유는 맥락으로 고려할 단어를 4개로 정했기 때문. \n",
    "\n",
    "---\n",
    "\n",
    "- 입력층 : 맥락에 포함시킬 단어가 N개라면 입력층도 N개이다.\n",
    "<br></br><br></br>\n",
    "- 은닉층 : 입력층의 여러개를 평균낸 값\n",
    "<br></br><br></br>\n",
    "- 출력층 : 뉴런의 개수 총 7개. 뉴런 하나하나가 각각의 단어에 대응된다. **출력층 뉴런은 각 단어의 '점수'를 뜻하며 값이 높을수록 대응 단어의 출현 확률도 높아진다.** 여기서 점수란 확률로 해석되기 전 값이고, 이 점수에 소프트맥수를 적용해서 확률을 얻을 수 있다.\n",
    "\n",
    "---\n",
    "\n",
    "입력층에서 은닉층으로의 변환을 완전연결계층 (가중치 $W_{in}$)이 처리한다. \n",
    "\n",
    "이때, (input이 7단어이고 hidden이 3개의 노드를 가질 때) 완전연결계층의 **가중치 $W_{in}$은 $7X3$ 행렬이며, <u>이 가중치의 각 행이 해당 단어의 분산 표현!</u>**\n",
    "\n",
    "        you - 0 0 0\n",
    "        say - 0 0 0\n",
    "    goodbye - 0 0 0\n",
    "        and - 0 0 0\n",
    "          I - 0 0 0\n",
    "      hello - 0 0 0\n",
    "          . - 0 0 0 \n",
    "          \n",
    "따라서 **학습을 진행할수록 맥락에서 출현하는 단어(target)를 잘 추측하는 방향으로 이 분산 표현들이 갱신**될 것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:56.386293Z",
     "start_time": "2020-01-06T06:25:56.371946Z"
    }
   },
   "outputs": [],
   "source": [
    "## CBOW 모델의 추론 처리, 맥락 2개 모델\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common_wj.layers import MatMul\n",
    "\n",
    "# 샘플 맥락 데이터 \n",
    "c0 = np.array([1,0,0,0,0,0,0])\n",
    "c1 = np.array([0,0,1,0,0,0,0])\n",
    "\n",
    "# 가중치 초기화\n",
    "W_in = np.random.randn(7,3)\n",
    "W_out = np.random.randn(3,7)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in) # 가중치 W_in을 공유한다\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h) # 각 단어의 점수 계산\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:20:20.955171Z",
     "start_time": "2020-01-02T07:20:20.790537Z"
    }
   },
   "source": [
    "## CBOW 모델의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW 모델은 출력층에서 각 단어의 점수를 출력했다. 이 점수에 소프트맥스 함수를 적용하면 **확률**을 얻을 수 있는데, \n",
    "\n",
    "이 확률은 <u>맥락 (전후 단어)이 주어졌었을 때 그 중앙에 어떤 단어가 출현하는 지 </u>를 나타낸다.\n",
    "\n",
    "CBOW의 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정하는 일을 한다. \n",
    "\n",
    "그 결과, 가중치 $W_{in}$ (또한 $W_{out}$ 에도) 에 **단어의 출현 패턴을 파악한 벡터가 학습**된다.\n",
    "\n",
    "이후 소프트맥스와 교차 엔트로피 오차를 통해 손실을 얻어 학습에 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec의 가중치와 분산 표현\n",
    "\n",
    "입력 측 완전연결계층의 가중치 ($W_{in}$)와 출력 측 완전연결계층의 가중치 ($W_{out}$) 둘 다 각 단어의 분산 표현을 가지고 있다.\n",
    "\n",
    "BUT $W_{out}$에는 단어의 분산표현이 열 방향(수직 방향)으로 저장된다.\n",
    "\n",
    "$W_{in} = 7X3, W_{out} = 3X7$ \n",
    "\n",
    "이때 우리가 활용할 수 있는 선택지는 다음의 세 가지가 있다.\n",
    "\n",
    "- A. 입력 측의 가중치만 이용\n",
    "- B. 출력 측의 가중치만 이용\n",
    "- C. 양쪽 가중치 모두 이용 \n",
    "\n",
    "**A안인 입력 측 가중치만 이용하는 것이 보편적 방안이다.**\n",
    "\n",
    "(참고) GloVe에서는 두 가중치를 더했을 때 더 좋은 결과를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 데이터 준비\n",
    "\n",
    "\"You say goodbye and I say hello.\"\n",
    "\n",
    "## 맥락과 타깃\n",
    "\n",
    "맥락은 타깃의 주변 단어로, 여러 단어가 될 수 있다.\n",
    "\n",
    "이때 맨 끝 단어는 고려하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:57.260093Z",
     "start_time": "2020-01-06T06:25:57.248097Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common_wj.util import preprocess\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:57.424078Z",
     "start_time": "2020-01-06T06:25:57.414601Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size = 1):\n",
    "    \n",
    "    target = corpus[window_size:-window_size] # 맥락의 개수가 채워지지 않는 양 끝 단어는 제외\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        cs = [] # context_per_target\n",
    "        for t in range(-window_size,window_size + 1): # target=0을 기준으로 window_size만큼 좌우 \n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:57.587231Z",
     "start_time": "2020-01-06T06:25:57.581265Z"
    }
   },
   "outputs": [],
   "source": [
    "contexts, target = create_contexts_target(corpus,)\n",
    "\n",
    "print(contexts) #\n",
    "print(\"맥락의 형상:\",contexts.shape,'\\n')\n",
    "print(target)\n",
    "print(\"타깃의 형상:\",target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원핫 표현으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:57.997975Z",
     "start_time": "2020-01-06T06:25:57.978209Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    \n",
    "    '''one-hot encoding 으로 변환 \n",
    "    \n",
    "    param corpus: 단어 ID목록(1차원 혹은 2차원의 NumPy배열)\n",
    "    param vocab_size: 어휘수(unique)\n",
    "    :return:one-hot표현(2차원 혹은 3차원의 NumPy배열)\n",
    "    ''' \n",
    "    N = corpus.shape[0]\n",
    "    \n",
    "    if corpus.ndim == 1: # target\n",
    "        one_hot = np.zeros((N, vocab_size), dtype = np.int32) # unique한 어휘 개수로 one hot length 부여됨.\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1 # 한 단어 당 하나의 one-hot\n",
    "            \n",
    "    elif corpus.ndim == 2: # contexts\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype = np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids): # word_id 개수만큼 다시 반복분돌기\n",
    "                one_hot[idx_0,idx_1,word_id] = 1\n",
    "                \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:58.143254Z",
     "start_time": "2020-01-06T06:25:58.121868Z"
    }
   },
   "outputs": [],
   "source": [
    "print(target)\n",
    "convert_one_hot(target, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:58.306973Z",
     "start_time": "2020-01-06T06:25:58.299962Z"
    }
   },
   "outputs": [],
   "source": [
    "print(contexts)\n",
    "convert_one_hot(contexts, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:30:48.208641Z",
     "start_time": "2020-01-06T15:30:48.039467Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common_wj.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size = 1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:13:21.373279Z",
     "start_time": "2020-01-06T16:13:21.343367Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common_wj.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "class simpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size # 인수로 어휘 수와 은닉층의 뉴런 수를 받는다.\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f') # 32비트 부동소수점 수\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in) # W_in은 contexts의 개수만큼 생성 (즉, window_size*2 만큼 생성)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "        \n",
    "    def forward(self, contexts, target): # 인수로 맥락과 타깃을 받아서 loss를 반환\n",
    "        # contexts.shape = (6, 2, 7), target.shape = (6, 7)\n",
    "        h0 = self.in_layer0.forward(contexts[:,0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:41:33.229019Z",
     "start_time": "2020-01-06T15:41:33.222222Z"
    }
   },
   "source": [
    "## 학습 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:13:13.683331Z",
     "start_time": "2020-01-06T16:13:13.670081Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleCBOW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1de78d79f1b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleCBOW' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common_wj.trainer import Trainer\n",
    "from common_wj.optimizer import Adam\n",
    "#from simple_cbow import SimpleCBOW\n",
    "import SimpleCB\n",
    "from common_wj.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target,vocab_size)\n",
    "contexts = convert_one_hot(contexts,vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
