{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션의 구조\n",
    "\n",
    "- seq2seq를 한층 더 강력하게 하는 **어텐션 매커니즘**\n",
    "- 인간처럼 필요한 정보에만 \"주목\"할 수 있게 만든다\n",
    "- seq2seq의 근본적인 문제를 해결한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq의 문제점\n",
    "\n",
    "- Encoder의 출력은 \"고정 길이의 벡터\"\n",
    "    - 고정 길이 벡터는 입력 문장의 길이에 관계없이 항상 같은 길이의 벡터로 변환하게 해준다\n",
    "    - **아무리 긴 벡터여도 고정 길이에 우겨넣어짐**\n",
    "\n",
    "<img src = \"../imgs/fig 8-1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 개선\n",
    "\n",
    "- Encoder 출력의 길이는 입력 문장에 길이에 따라 바꿔주는 것이 좋다! (당연)\n",
    "- 구체적으로는, **시각별 LSTM 계층의 은닉 상태 벡터를 모두 이용하는 것**\n",
    "\n",
    "<img src = \"../imgs/fig 8-2.png\" width=\"500\">\n",
    "\n",
    "\n",
    "#### <center>===> 이것으로 Encoder는 \"하나의 고정 길이 벡터\"라는 제약에서 벗어남</center>\n",
    "\n",
    "- (참고) RNN 계층을 초기화할 때 두 가지 반환 중 선택 가능\n",
    "    - 모든 시각의 은닉 상태 벡터 반환 \n",
    "        - [keras] `return_sequences=True`\n",
    "    - 마지막 은닉 상태 벡터만 반환\n",
    "    \n",
    "    \n",
    "- 각 시각의 은닉 상태에는 직전에 입력된 단어에 대한 정보가 많이 포함되어 있다.\n",
    "    - 예를 들어 `나` `는` `고양이` `로소` `이다` 에서 `고양이` 벡터에는 `나` `는` `고양이` 의 정보가 담겨 있다\n",
    "    - 주변 정보를 균형있게 담기 위해서는 **양방향 RNN** 이 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 개선 ①\n",
    "\n",
    "<img src = \"../imgs/fig 8-5.png\" width=\"700\">\n",
    "\n",
    "- 앞 장의 Decoder는 Encoder의 LSTM 계층의 마지막 은닉 상태만을 이용한다 \n",
    "- hs에서 마지막 줄만 빼내어 Decoder에 전달한 것\n",
    "\n",
    "#### ===> hs 전부를 활용할 수 있도록 개선\n",
    "\n",
    "---\n",
    "\n",
    "- 사람이 문장 `나는 고양이로소이다`를 영어로 번역한다면? \n",
    "    - 나 = I, 고양이 = cat\n",
    "    - 나와 고양이가 문장의 주를 이루므로 나와 고양이라는 단어에 \"주목\" 하면서 번역을 하게 됨. 나 = I, 고양이 = cat 와 같은 단어 간 대응 관계(`alignment`)를 미리 알고 있다면 번역 효과가 더 좋을 것임.\n",
    "    - **대응 관계 (`alignment`) 아이디어 를 활용하는 것이 바로 어텐션 매커니즘**\n",
    "    \n",
    "---\n",
    "\n",
    "#### 입력과 출력의 단어 간 대응 관계를 seq2seq에게 학습시키자!!!\n",
    "\n",
    "- `도착어 단어`와 `대응 관계`에 있는 `출발어 단어`의 정보를 골라내면 더 학습(번역)이 잘 될 것 이다!\n",
    "- **필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 목표**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src = \"../imgs/fig 8-6.png\" width=\"700\">\n",
    "\n",
    "- `어떤 계산`이 받는 입력 두 가지\n",
    "    1. Encoder로부터 받는 hs\n",
    "    2. 시각 별 LSTM 계층의 은닉 상태\n",
    "       - 여기서 필요한 정보만 골라 위쪽의 Affine계층으로 출력한다\n",
    "- Decoder의 첫번째 계층에 마지막 은닝 상태 벡터 hs를 전달하는 것은 유지\n",
    "\n",
    "- 목적은 **단어들의 대응관계 추출**\n",
    "    - 각 시각에서 Decoder에 입력된 단어와 대응관계인 단어의 벡터를 hs에서 골라내자.\n",
    "    - 예를 들어 Decoder가 `I`를 출력할 때 hs에서 `나`에 대응하는 벡터를 선택함.\n",
    "        - 그러나 일부 벡터만 선택하는 것은 미분이 가능하지 않다. ( = 오차역전파 안됨 )\n",
    "        - #### 일부 선택이 아니라 **모두 선택**하자. 대신 **가중치**를 별도로 계산\n",
    "\n",
    "\n",
    "<img src = \"../imgs/fig 8-8.png\" width=\"600\">\n",
    "\n",
    "- 각 단어의 중요도를 나타내는 가중치 $a$\n",
    "- a는 0.0 ~ 1.0 사이의 스칼라 값이며 모든 원소의 총합은 1이다\n",
    "- 각 단어의 중요성을 보여주는 가중치 a와 각 단어 벡터 hs로부터 가중 합 Weighted Sum을 구하여 원하는 벡터 얻기!\n",
    "    - 그 결과를 `맥락 벡터`라고 부르고 기호로는 c로 표기한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "[0.6241871  1.03772847 0.41861277 0.15210266]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T,H = 5,4\n",
    "hs = np.random.randn(T,H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5,1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar \n",
    "print(t.shape)\n",
    "\n",
    "# 단어 벡터 가중치 곱해서 합친 결과\n",
    "c = np.sum(t, axis=0) \n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# 미니배치 처리용 가중합\n",
    "N, T, H  = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1) # 1번 인덱스 축을 지워라\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [],[]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H  = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis = 1) #sum의 역전파\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2) # repeat의 역전파\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 개선 ②\n",
    "\n",
    "#### <center><u> 가중치 a는 어떻게 구하나? </u></center>\n",
    "<img src = \"../imgs/fig 8-12.png\" width=\"400\">\n",
    "\n",
    "- h : Decoder 계층의 은닉 상태\n",
    "- **h가 hs의 각 단어 벡터와 얼마나 비슷한가**를 계산\n",
    "    - using 내적\n",
    "    - 두 벡터 간 내적 $ a{\\cdot}b $ : \"두 벡터가 얼마나 같은 방향을 향하고 있는가\"\n",
    "    - <img src = \"../imgs/fig 8-13.png\" width=\"400\">\n",
    "    - hs 와 h 을 내적하여 각 단어 벡터와의 유사도 구함\n",
    "    - s는 이후 소프트맥스로 0.0~1.0 사이 확률값으로 치환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10,5,4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 개선 ③\n",
    "\n",
    "<img src= \"../imgs/fig 8-16 fixed.png\" width=\"600\">\n",
    "\n",
    "1. `Attention Weight` 계층 : Encoder의 출력인 각 단어 벡터 hs에 주목하여 가중치 a 구함\n",
    "2.` Weight Sum` 계층 : a와 hs의 가중합을 구하고 맥락 벡터 c 출력\n",
    "\n",
    "**===> `Attention` 계층**\n",
    "\n",
    "<img src= \"../imgs/fig 8-17.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        # 1. 가중치 구하기\n",
    "        self.attention_weight_layer = AttentionWeight() \n",
    "        # 2. 가중합 구하기\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"../imgs/fig 8-18.png\" width=\"700\">\n",
    "\n",
    "- **LSTM 계층과 Affine 계층 사이에 Attention 계층 삽입**\n",
    "\n",
    "<img src= \"../imgs/fig 8-19.png\" width=\"500\">\n",
    "\n",
    "- [LSTM + Attention] 연결 벡터를 Affine 계층에 입력\n",
    "\n",
    "---\n",
    "\n",
    "**`Time Attention`**\n",
    "\n",
    "<img src= \"../imgs/fig 8-20.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 방향으로 펼쳐진 다수의 Attention 계층 구현\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        # 각 Attention 계층의 각 단어 가중치 보관\n",
    "        self.attention_weights = []\n",
    "\n",
    "        # Attention 계층을 필요한 수만큼 만들기\n",
    "        for t in range(T): \n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 구현\n",
    "\n",
    "기존의 Encoder 클래스와 유사하다\n",
    "\n",
    "#### 기존의 forward() LSTM 계층 마지막 은닉 상태 반환 ====> 모든 은닉 상태 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from common.time_layers import *\n",
    "from ch07_RNN을_사용한_문장_생성.seq2seq import Encoder, Seq2seq\n",
    "from ch08_어텐션.attention_layer import TimeAttention\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "#         return hs[:, -1, :]  # 마지막 상태의 은닉 상태만 추출\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):  # Decoder에서 넘어온 기울기 dh.\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 구현\n",
    "\n",
    "<img src = \"../imgs/fig 8-21.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 초기화\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 매개변수 초기화 및 계층 생성\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f') # 2개 받음\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention() # 추가된 부분\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        self.params, self.grads = [], []\n",
    "\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    ##############\n",
    "    # 학습 담당\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1] # hs의 마지막줄만 LSTM에 넘기기\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        \n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore) #\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1 ## 2개 받아오는 부분\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    ##############\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1] # 추가된 부분. hs의 마지막 단만을 받아오기\n",
    "        self.lstm.set_state(h)  # Encoder 출력 h를 Decoder 계층의 상태로 설정\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq 구현\n",
    "\n",
    "- seq2seq와 거의 유사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch07_RNN을_사용한_문장_생성.seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 초기화\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args) # new\n",
    "        self.decoder = AttentionDecoder(*args) # new\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜 형식 변환 문제 \n",
    "\n",
    "<img src = \"../imgs/fig 8-23.png\" width=\"400\">\n",
    "\n",
    "- 입력 문장의 길이 통일하기 위해 공백 문자로 패딩\n",
    "- 구분 문자는 `_`\n",
    "- 출력의 문자 수는 일정하므로 출력 구분문자는 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 10[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 23[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 33[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 43[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 56[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 65[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 77[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 86[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 96[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 105[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 116[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 126[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 135[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 145[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 155[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 165[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 175[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "정확도 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 9[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 19[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 28[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 38[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 47[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 56[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 66[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 75[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 85[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 94[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 104[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 113[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 123[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 133[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 142[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 152[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 161[s] | loss 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-11-08\n",
      "---\n",
      "정확도 51.640%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 9[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 19[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 29[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 38[s] | loss 0.09\n",
      "| epoch 3 |  iter 101 / 351 | time 48[s] | loss 0.07\n",
      "| epoch 3 |  iter 121 / 351 | time 57[s] | loss 0.05\n",
      "| epoch 3 |  iter 141 / 351 | time 67[s] | loss 0.04\n",
      "| epoch 3 |  iter 161 / 351 | time 77[s] | loss 0.03\n",
      "| epoch 3 |  iter 181 / 351 | time 88[s] | loss 0.03\n",
      "| epoch 3 |  iter 201 / 351 | time 99[s] | loss 0.02\n",
      "| epoch 3 |  iter 221 / 351 | time 109[s] | loss 0.02\n",
      "| epoch 3 |  iter 241 / 351 | time 120[s] | loss 0.02\n",
      "| epoch 3 |  iter 261 / 351 | time 131[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 142[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 155[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 166[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 177[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 10[s] | loss 0.01\n",
      "| epoch 4 |  iter 41 / 351 | time 19[s] | loss 0.01\n",
      "| epoch 4 |  iter 61 / 351 | time 30[s] | loss 0.01\n",
      "| epoch 4 |  iter 81 / 351 | time 40[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 50[s] | loss 0.01\n",
      "| epoch 4 |  iter 121 / 351 | time 60[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 70[s] | loss 0.01\n",
      "| epoch 4 |  iter 161 / 351 | time 80[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 111[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 123[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 140[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 152[s] | loss 0.00\n",
      "| epoch 4 |  iter 301 / 351 | time 163[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 174[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 186[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 10[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 20[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 30[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 40[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 49[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 59[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 69[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 79[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 110[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 120[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 130[s] | loss 0.00\n",
      "| epoch 5 |  iter 281 / 351 | time 140[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 151[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 161[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 172[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 6 |  iter 21 / 351 | time 11[s] | loss 0.00\n",
      "| epoch 6 |  iter 41 / 351 | time 22[s] | loss 0.00\n",
      "| epoch 6 |  iter 61 / 351 | time 32[s] | loss 0.00\n",
      "| epoch 6 |  iter 81 / 351 | time 42[s] | loss 0.00\n",
      "| epoch 6 |  iter 101 / 351 | time 52[s] | loss 0.00\n",
      "| epoch 6 |  iter 121 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 6 |  iter 141 / 351 | time 71[s] | loss 0.00\n",
      "| epoch 6 |  iter 161 / 351 | time 81[s] | loss 0.00\n",
      "| epoch 6 |  iter 181 / 351 | time 91[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 101[s] | loss 0.00\n",
      "| epoch 6 |  iter 221 / 351 | time 111[s] | loss 0.00\n",
      "| epoch 6 |  iter 241 / 351 | time 121[s] | loss 0.00\n",
      "| epoch 6 |  iter 261 / 351 | time 130[s] | loss 0.00\n",
      "| epoch 6 |  iter 281 / 351 | time 140[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 150[s] | loss 0.00\n",
      "| epoch 6 |  iter 321 / 351 | time 160[s] | loss 0.00\n",
      "| epoch 6 |  iter 341 / 351 | time 170[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 7 |  iter 21 / 351 | time 10[s] | loss 0.00\n",
      "| epoch 7 |  iter 41 / 351 | time 19[s] | loss 0.00\n",
      "| epoch 7 |  iter 61 / 351 | time 29[s] | loss 0.00\n",
      "| epoch 7 |  iter 81 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 48[s] | loss 0.00\n",
      "| epoch 7 |  iter 121 / 351 | time 58[s] | loss 0.00\n",
      "| epoch 7 |  iter 141 / 351 | time 67[s] | loss 0.00\n",
      "| epoch 7 |  iter 161 / 351 | time 77[s] | loss 0.00\n",
      "| epoch 7 |  iter 181 / 351 | time 86[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 96[s] | loss 0.00\n",
      "| epoch 7 |  iter 221 / 351 | time 106[s] | loss 0.00\n",
      "| epoch 7 |  iter 241 / 351 | time 116[s] | loss 0.00\n",
      "| epoch 7 |  iter 261 / 351 | time 126[s] | loss 0.00\n",
      "| epoch 7 |  iter 281 / 351 | time 137[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 147[s] | loss 0.00\n",
      "| epoch 7 |  iter 321 / 351 | time 157[s] | loss 0.00\n",
      "| epoch 7 |  iter 341 / 351 | time 169[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 8 |  iter 21 / 351 | time 10[s] | loss 0.00\n",
      "| epoch 8 |  iter 41 / 351 | time 21[s] | loss 0.00\n",
      "| epoch 8 |  iter 61 / 351 | time 31[s] | loss 0.00\n",
      "| epoch 8 |  iter 81 / 351 | time 42[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 51[s] | loss 0.00\n",
      "| epoch 8 |  iter 121 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 8 |  iter 141 / 351 | time 71[s] | loss 0.00\n",
      "| epoch 8 |  iter 161 / 351 | time 81[s] | loss 0.00\n",
      "| epoch 8 |  iter 181 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 8 |  iter 221 / 351 | time 110[s] | loss 0.00\n",
      "| epoch 8 |  iter 241 / 351 | time 121[s] | loss 0.00\n",
      "| epoch 8 |  iter 261 / 351 | time 130[s] | loss 0.00\n",
      "| epoch 8 |  iter 281 / 351 | time 140[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 149[s] | loss 0.00\n",
      "| epoch 8 |  iter 321 / 351 | time 159[s] | loss 0.00\n",
      "| epoch 8 |  iter 341 / 351 | time 168[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.960%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 9 |  iter 21 / 351 | time 15[s] | loss 0.00\n",
      "| epoch 9 |  iter 41 / 351 | time 26[s] | loss 0.00\n",
      "| epoch 9 |  iter 61 / 351 | time 36[s] | loss 0.00\n",
      "| epoch 9 |  iter 81 / 351 | time 46[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 56[s] | loss 0.00\n",
      "| epoch 9 |  iter 121 / 351 | time 66[s] | loss 0.00\n",
      "| epoch 9 |  iter 141 / 351 | time 77[s] | loss 0.00\n",
      "| epoch 9 |  iter 161 / 351 | time 87[s] | loss 0.00\n",
      "| epoch 9 |  iter 181 / 351 | time 108[s] | loss 0.00\n",
      "| epoch 9 |  iter 201 / 351 | time 121[s] | loss 0.00\n",
      "| epoch 9 |  iter 221 / 351 | time 136[s] | loss 0.00\n",
      "| epoch 9 |  iter 241 / 351 | time 152[s] | loss 0.00\n",
      "| epoch 9 |  iter 261 / 351 | time 168[s] | loss 0.00\n",
      "| epoch 9 |  iter 281 / 351 | time 180[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 195[s] | loss 0.00\n",
      "| epoch 9 |  iter 321 / 351 | time 206[s] | loss 0.00\n",
      "| epoch 9 |  iter 341 / 351 | time 216[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.960%\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 10 |  iter 21 / 351 | time 9[s] | loss 0.00\n",
      "| epoch 10 |  iter 41 / 351 | time 18[s] | loss 0.00\n",
      "| epoch 10 |  iter 61 / 351 | time 27[s] | loss 0.00\n",
      "| epoch 10 |  iter 81 / 351 | time 36[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 46[s] | loss 0.00\n",
      "| epoch 10 |  iter 121 / 351 | time 55[s] | loss 0.00\n",
      "| epoch 10 |  iter 141 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 10 |  iter 161 / 351 | time 73[s] | loss 0.00\n",
      "| epoch 10 |  iter 181 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 92[s] | loss 0.00\n",
      "| epoch 10 |  iter 221 / 351 | time 101[s] | loss 0.00\n",
      "| epoch 10 |  iter 241 / 351 | time 110[s] | loss 0.00\n",
      "| epoch 10 |  iter 261 / 351 | time 119[s] | loss 0.00\n",
      "| epoch 10 |  iter 281 / 351 | time 128[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 137[s] | loss 0.00\n",
      "| epoch 10 |  iter 321 / 351 | time 146[s] | loss 0.00\n",
      "| epoch 10 |  iter 341 / 351 | time 155[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.960%\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../ch07_RNN을_사용한_문장_생성/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from ch07_RNN을_사용한_문장_생성.seq2seq import Seq2seq\n",
    "from ch07_RNN을_사용한_문장_생성.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전 --- 성능 향상 기법 1\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention을 적용한 seq2seq모델이 baseline + peeky 모델보다 학습속도 면과 정확도 면에서 우세함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 시각화\n",
    "\n",
    "- 어텐션 계층은 각 시각의 어텐션 가중치를 인스턴스 변수로 보관하고 있음!\n",
    "    - `attention_weights`\n",
    "    \n",
    "    \n",
    "**학습된 모델을 사용하여 시계열 변환 수행했을 때의 어텐션 가중치 시각화**\n",
    "- 가로축 : 입력 문장\n",
    "- 세로축 : 출력 문장\n",
    "- 맵의 각 원소는 밝을 수록 값이 크다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시드 고정\n",
    "np.random.seed(1984) \n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션에 관한 남은 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 양방향 RNN\n",
    "\n",
    "===> 단어의 주변 정보를 균형있게 담을 수 있다!\n",
    "\n",
    "<img src = \"../imgs/fig 8-30.png\" width=\"600\">\n",
    "\n",
    "- 각 단어에 대응하는 은닉 상태 벡터에는 **좌와 우 양쪽 방향으로부터 균형있게 정보를 인코딩!**\n",
    "\n",
    "    - LSTM 계층 + 역방향 LSTM 계층\n",
    "    - 두 LSTM 계층의 은닉 상태를 연결 시킨 벡터를 최종 은닉 상태로 처리\n",
    "\n",
    "\n",
    "- LSTM 계층\n",
    "    - 입력 문장 왼 ==> 오\n",
    "\n",
    "\n",
    "- 역방향 LSTM 계층\n",
    "    - 입력 문장 오 ==> 왼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention 계층 사용 방법\n",
    "\n",
    "### 방법 1. - LSTM - Attention - Affine\n",
    "\n",
    "<img src = \"../imgs/fig 8-31.png\" width=\"400\" align='center'>\n",
    "\n",
    "- Affine 계층이 맥락 벡터 이용\n",
    "- 구현이 더 쉽다\n",
    "\n",
    "### 방법 2. - LSTM - Attention - LSTM\n",
    "\n",
    "<img src = \"../imgs/fig 8-32.png\" width=\"450\" align='center'>\n",
    "\n",
    "- LSTM 계층이 맥락 벡터 이용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq 심층화와 skip 연결\n",
    "\n",
    "<img src = \"../imgs/fig 8-33.png\" width=\"600\" align='center'>\n",
    "\n",
    "- Encoder와 Decoder 3층 LSTM 계층 사용\n",
    "\n",
    "- **Encoder와 Decoder는 같은 층수의 LSTM 계층을 이용하는 것이 일반적임**    \n",
    "\n",
    "---\n",
    "\n",
    "- **층이 깊어질 수록 드롭아웃 혹은 가중치 공유 등의 기술 사용 권장**\n",
    "\n",
    "- `skip connection` : **층을 깊게 할 때 사용되는 기법** 중 하나    \n",
    "    - `residual connection` or `shot-chut`\n",
    "    - <img src = \"../imgs/fig 8-34.png\" width=\"200\" align='center'>\n",
    "    - 계층을 건너뛰는 기법\n",
    "        - 덧셈 연결부에 출력 2개가 더해진다 \n",
    "        - 덧셈은 역전파 시 기울기를 그대로 흘려보내므로, 층이 깊어져도 기울기가 소실되지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구글 신경망 기계 번역 (Google Neural Machine Translation, GNMT)\n",
    "\n",
    "<img src = \"../imgs/fig 8-35.png\" width=\"600\" align='center'>\n",
    "\n",
    "- LSTM 계층 다층화\n",
    "- Encoder의 첫번째 계층만 양방향 LSTM \n",
    "- skip connection\n",
    "- 다수 GPU로 분산 학습 (100개 gpu로 6일..)\n",
    "\n",
    "==> 사람의 정확도에 근사함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트랜스포머\n",
    "\n",
    "RNN은 이전 시각에 계산한 결과를 이용하여 순차적으로 계산하는 방식. 그러나 이러한 계산은 병렬 처리가 불가하다는 치명적인 단점이 있음. 이에 RNN을 없애는 연구 ( 병렬 계산할 수 있는 RNN 연구 ) 가 활발히 이뤄짐 그 중 하나가    \n",
    "\n",
    "### *Attention is all you need* 에서 제안한 기법인 `Transformer` 모델\n",
    "\n",
    "- RNN이 아닌 어텐션을 사용하여 처리\n",
    "- **`Self-Attention` : 자기 자신에 대한 주목**\n",
    "- **하나의 시계열 데이터 내에서** 각 원소가 다른 원소들과 어떻게 관련되는지를 보자.\n",
    "\n",
    "<img src = \"../imgs/fig 8-37.png\" width=\"500\" align='center'>\n",
    "\n",
    "- 왼) 서로 다른 두 시계열 데이터가 입력됨\n",
    "- 오) 두 입력이 하나의 시계열 데이터로부터 나온다\n",
    "    - 하나의 시계열 데이터 내에서 원소 간 대응 관계 구함\n",
    "    \n",
    "---\n",
    "\n",
    "<img src = \"../imgs/fig 8-38.png\" width=\"500\" align='center'>\n",
    "\n",
    "- Encoder와 Decoder 모두에서 RNN 대신 Attention 사용\n",
    "    - 학습 시간 대폭 감소!\n",
    "    - 병렬 처리 가능!\n",
    "    - 정확도 향상!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴럴 튜링 머신 (Neural Turing Machine, NTM)\n",
    "\n",
    "**외부 메모리**를 통해 RNN의 한정적인 정보 보관량(고정 길이) 을 늘리는 방식\n",
    "\n",
    "<img src = \"../imgs/fig 8-40.png\" width=\"300\" align='center'>\n",
    "\n",
    "- 바깥에 메모리! ==> 중앙 컨트롤러 (RNN 신경망) 에게 컴퓨터 능력을 줌\n",
    "    - 정보 쓰기 \n",
    "    - 정보 지우기\n",
    "    - 정보 읽기\n",
    "\n",
    "---\n",
    "<img src = \"../imgs/fig 8-41.png\" width=\"600\" align='center'>\n",
    "\n",
    "\n",
    "- **어텐션을 이용해** Write Head & Read Head 계층이 메모리 조작\n",
    "- 읽고 쓰는 작업을 가중치로 대체하여..\n",
    "- 2개의 어텐션 이용\n",
    "    - 콘텐츠 기반 어텐션\n",
    "        - 그냥 어텐션\n",
    "        - query 벡터와 비슷한 벡터를 메모리에서 찾아내줌\n",
    "    - 위치 기반 어텐션\n",
    "        - 이전 시각에서 주목한 메모리의 위치 기준으로 단어 전후로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
