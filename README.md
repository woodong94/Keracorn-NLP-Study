# Deep Learning from Scratch2
**밑바닥부터 시작하는 딥러닝 2**

> 스터디 시작일자 : 2019.12.15

> 스터디 목적 : 자연어 처리에 관해 심화 학습하면서 밑바닥부터 구현해보기

<br/>
<p align="left">
<img src ="http://www.hanbit.co.kr/data/books/B8950212853_l.jpg" height="250px"/>
<!-- #</p> -->
<br/>

## 목차

|   | Chapter                   | 개인 진도   | 발제 |
|---|---------------------------|----------|-------|
| 1 | 신경망 복습                  | 19.12.18 |       |
| 2 | 자연어와 단어의 분산 표현       | 19.12.27 |  백승주  |
| 3 | word2vec                  | 20.1.7   | 김우정  |
| 4 | word2vec 속도 개선          | 20.2.22  |  조원익  |
| 5 | 순환 신경망 (RNN)            |          |  운봉영  |
| 6 | 게이트가 추가된 RNN           |          | 김희아   |
| 7 | RNN을 사용한 문장 생성        |          |  백승주  |
| 8 | 어텐션                     |          |  김우정  |

## 참여자
슬기, 봉영, 원익, 승주, 우정, 희아

## 스터디 일지
**Week 1.** Ch 1. 신경망 복습
- 스터디 시작 및 방법 논의 
    - 각 챕터의 발제자를 정해서 오프라인 모임 때 발표 
    - 다음 오프라인 모임까지 ch.1 ~ ch4
   
**Week 2.** Ch 2. 자연어와 단어의 분산 표현 - Ch 3. word2vec
- ch2. SVD의 개념 심화 논의 
    - 2-1. eigenvalue 의미?
    - 2-2. singular vector -> 중요도 큰 값이 왜 중요한 것인가??

**Week 3.** Ch4. word2vec 속도 개선 - Ch 5. RNN 
- Ch 4. word2vec 속도 개선 (by. 원익)
    - analogy test의 원리에 관해 심화 공부 진행
    - Ethayarajh et al., Towards Understanding Linear Word Analogies, ACL 2019
- Ch 5. RNN (by. 봉영)


---  
> 🤖 스터디 외 부가 활동   
- 2020.01.20 Naver AI Burning Day 해커톤 참가 (우정, 희아, 박xx)  
- 2020.01.31 본선 진출 (top 30)
- 2020.02.13 - 2020.02.15 2박 3일 본선 in 춘천 네이버 커넥트원
---
- 2020.02.13 인공지능 데이터셋 경진대회 제안서 제출  
- 2020.02.24 1차 서류 평가 통과, 우수팀 10팀 선정
---
