{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T19:20:38.365049Z",
     "start_time": "2019-12-27T19:20:38.361839Z"
    }
   },
   "source": [
    "# 추론 기반 기법과 신경망\n",
    "\n",
    "\n",
    "## 통계 기반 기법의 문제점\n",
    "\n",
    "통계 기반 기법의 문제점은 대규모 말뭉치를 다룰 때 발생한다.\n",
    "\n",
    "예를 들어 말뭉치 어휘의 수가 100만개라면, 100만X100만의 거대 희소 행렬에 SVD ($O(n^3)$)를 적용해야하는데 이는 현실적으로 불가능.\n",
    "\n",
    "- `통계 기반 기법` : 학습 데이터를 한꺼번에 처리 (배치 학습)\n",
    "- `추론 기반 기법` : 학습 데이터의 일부를 사용하여 순차적으로 학습(미니배치 학습)\n",
    "\n",
    "미니배치 학습은 계산량이 큰 작업을 처리할 때 효율적. 데이터를 작게 나눠 학습하기 때문에 연산 가능. 또한 병렬 계산도 가능하게 하여 학습 속도 또한 높일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T19:21:48.195984Z",
     "start_time": "2019-12-27T19:21:48.187181Z"
    }
   },
   "source": [
    "## 추론 기반 기법 개요\n",
    "```\n",
    "you ? goodbye and I say hello.\n",
    "```\n",
    "문장에서 `?` 을 추론하는 작업. 모델은 이러한 추론 문제를 반복해서 풀면서 **단어의 출현 패턴을 학습**하여 **각 단어 별로 출현 확률을 출력**한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망에서의 단어 처리\n",
    "\n",
    "신경망을 이용해 단어를 처리하기 위해서는 단어를 **고정 길이의 벡터**로 변환해야 한다. (신경망의 입력층에서 뉴런의 수를 고정해야 함)\n",
    "\n",
    "대표적인 방법이 `원핫 표현 (one-hot encoding)`\n",
    "\n",
    "입력층의 각 뉴런이 각 단어에 대응됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:56.072982Z",
     "start_time": "2020-01-06T06:25:55.844007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79430496 1.43159989 0.45691755]\n"
     ]
    }
   ],
   "source": [
    "# 완전연결계층에 의한 원핫 인코딩 형식의 단어 변환\n",
    "# 7개의 단어가 입력으로 들어가고, 이는 3개의 은닉층 뉴런을 통해 변환되어짐\n",
    "import numpy as np\n",
    "\n",
    "c = np.array([1, 0, 0, 0, 0, 0, 0]) # 원 핫 입력\n",
    "W = np.random.randn(7,3) # 가중치\n",
    "h = np.matmul(c,W) # 중간 노드\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:24:10.482331Z",
     "start_time": "2020-01-02T06:24:10.469106Z"
    }
   },
   "source": [
    "원핫 표현의 단어 ID에 대응하는 원소만 1이고 그 외에는 0인 벡터.\n",
    "\n",
    "즉, c와 W의 행렬 곱은 **가중치의 행벡터 하나**를 뽑아낸 것과 같다.\n",
    "\n",
    "때문에 비효율적인 계산 방식. (4장에서 개선)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단순한 word2vec\n",
    "\n",
    "## CBOW 모델의 추론 처리\n",
    "\n",
    "CBOW <sup>continuous bag-of-words<sup>\n",
    "    \n",
    "CBOW 모델은 **맥락으로부터 타깃(target)을 추측**하는 용도의 신경망이다. \n",
    "\n",
    "- 타깃 : 중앙 단어\n",
    "- 맥락 : 중앙 단어의 주변 단어들\n",
    "\n",
    "<u>맥락이 입력이고 타깃이 출력이다</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:35:54.099273Z",
     "start_time": "2020-01-02T06:35:54.087577Z"
    }
   },
   "source": [
    "<img src=\"../imgs/CBOW.png\" width=\"300\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:39:05.797096Z",
     "start_time": "2020-01-02T06:39:05.792535Z"
    }
   },
   "source": [
    "위 그림을 보면 입력층이 4개 있고, 은닉층을 거쳐 출력층에 도달한다. 4개의 입력층에서 은닉층으로의 변환은 완전연결계층 $W_{in}$ 이 처리한다. 그리고 은닉층에서 출력층 뉴런으로의 변환은 다른 완전연결계층 $W_{out}$이 처리한다. 위 그림에서 입력층이 4개인 이유는 맥락으로 고려할 단어를 4개로 정했기 때문. \n",
    "\n",
    "---\n",
    "\n",
    "- 입력층 : 맥락에 포함시킬 단어가 N개라면 입력층도 N개이다.\n",
    "<br></br>\n",
    "- 은닉층 : 입력층 전체를 평균낸 값. (SUM / 입력층의 개수)\n",
    "<br></br>\n",
    "- 출력층 : 뉴런의 개수 총 7개. 뉴런 하나하나가 각각의 단어에 대응된다. **출력층 뉴런은 각 단어의 '점수'를 뜻하며 값이 높을수록 대응 단어의 출현 확률도 높아진다.** 여기서 점수란 확률로 해석되기 전 값이고, 이 점수에 소프트맥수를 적용해서 확률을 얻을 수 있다.\n",
    "\n",
    "---\n",
    "\n",
    "입력층에서 은닉층으로의 변환을 완전연결계층 (가중치 $W_{in}$)이 처리한다. \n",
    "\n",
    "이때, (input이 7단어이고 hidden이 3개의 노드를 가질 때) 완전연결계층의 <u>**가중치 $W_{in}$은 $7X3$ 행렬이며, 이 가중치의 각 행이 해당 단어의 분산 표현!</u>**\n",
    "\n",
    "        you - 0 0 0\n",
    "        say - 0 0 0\n",
    "    goodbye - 0 0 0\n",
    "        and - 0 0 0\n",
    "          I - 0 0 0\n",
    "      hello - 0 0 0\n",
    "          . - 0 0 0 \n",
    "          \n",
    "은닉층의 뉴런 수를 입력층의 뉴런 수보다 적게 하는 것이 중요한 핵심이다! 이렇게 해야 은닉층에는 단어 예측에 필요한 정보를 \"간결하게\" 담게 되며, 결과적으로 밀집벡터 표현을 얻을 수 있다.\n",
    "          \n",
    "따라서 **학습을 진행할수록 맥락에서 출현하는 단어(target)를 잘 추측하는 방향으로 이 분산 표현들이 갱신**될 것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:56.386293Z",
     "start_time": "2020-01-06T06:25:56.371946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11758873  0.00227049 -0.75579113 -0.22458675 -0.3500984  -0.63682774\n",
      " -0.63111001]\n"
     ]
    }
   ],
   "source": [
    "## CBOW 모델의 추론 처리, 맥락 2개 모델\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "# 샘플 맥락 데이터\n",
    "# 단어는 7개라고 가정한다.\n",
    "c0 = np.array([1,0,0,0,0,0,0]) \n",
    "c1 = np.array([0,0,1,0,0,0,0])\n",
    "\n",
    "# 가중치 초기화\n",
    "W_in = np.random.randn(7,3)\n",
    "W_out = np.random.randn(3,7)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in) # 가중치 W_in을 공유한다\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h) # 각 단어의 점수 계산\n",
    "\n",
    "print(s)\n",
    "# [ 0.11758873  0.00227049 -0.75579113 -0.22458675 -0.3500984  -0.63682774\n",
    "#  -0.63111001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:20:20.955171Z",
     "start_time": "2020-01-02T07:20:20.790537Z"
    }
   },
   "source": [
    "## CBOW 모델의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW 모델은 출력층에서 각 단어의 점수를 출력했다. 이 점수에 소프트맥스 함수를 적용하면 **확률**을 얻을 수 있는데, \n",
    "\n",
    "이 확률은 <u>맥락 (전후 단어)이 주어졌었을 때 그 중앙에 어떤 단어가 출현하는 지 </u>를 나타낸다.\n",
    "\n",
    "CBOW 모델의 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정하는 일을 한다. \n",
    "\n",
    "그 결과, 가중치 $W_{in}$ (또한 $W_{out}$ 에도) 에 **단어의 출현 패턴을 파악한 벡터가 학습**된다.\n",
    "\n",
    "이후 소프트맥스계층과 교차 엔트로피 오차 계층을 통해 손실을 얻어 학습에 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec의 가중치와 분산 표현\n",
    "\n",
    "입력 측 완전연결계층의 가중치 ($W_{in}$)와 출력 측 완전연결계층의 가중치 ($W_{out}$) 둘 다 각 단어의 분산 표현을 가지고 있다.\n",
    "\n",
    "BUT $W_{out}$에는 단어의 분산표현이 열 방향(수직 방향)으로 저장된다.\n",
    "\n",
    "$W_{in} = 7X3, W_{out} = 3X7$ \n",
    "\n",
    "이때 우리가 활용할 수 있는 선택지는 다음의 세 가지가 있다.\n",
    "\n",
    "- A. 입력 측의 가중치만 이용 - $W_{in}$\n",
    "- B. 출력 측의 가중치만 이용 - $W_{out}$\n",
    "- C. 양쪽 가중치 모두 이용 - ($W_{in},W_{out}$)\n",
    "\n",
    "**A안인 입력 측 가중치만 이용하는 것이 보편적 방안이다.**\n",
    "\n",
    "(참고) GloVe에서는 두 가중치를 더했을 때 더 좋은 결과를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 데이터 준비\n",
    "\n",
    "\"You say goodbye and I say hello.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맥락과 타깃\n",
    "\n",
    "맥락은 타깃의 주변 단어로, 여러 단어가 될 수 있다.\n",
    "\n",
    "이때 맨 끝 단어는 고려하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:57.260093Z",
     "start_time": "2020-01-06T06:25:57.248097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)\n",
    "# [0 1 2 3 4 1 5 6]\n",
    "# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:57.424078Z",
     "start_time": "2020-01-06T06:25:57.414601Z"
    }
   },
   "outputs": [],
   "source": [
    "# 말뭉치에서 맥락과 타깃을 만드는 코드\n",
    "def create_contexts_target(corpus, window_size = 1):\n",
    "    \n",
    "    target = corpus[window_size:-window_size] # 맥락의 개수가 채워지지 않는 양 끝 단어는 제외\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus) - window_size):\n",
    "        cs = [] # context_per_target\n",
    "        for t in range(-window_size,window_size + 1): # target=0을 기준으로 window_size만큼 좌우 \n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:57.587231Z",
     "start_time": "2020-01-06T06:25:57.581265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "맥락의 형상: (6, 2) \n",
      "\n",
      "[1 2 3 4 1 5]\n",
      "타깃의 형상: (6,)\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus,)\n",
    "\n",
    "print(contexts) # 맥락은 2차원 배열. contexts[0]에는 0번째 맥락이 저장되고,  contexts[1]에는 1번째 맥락이 저장되는 방식.\n",
    "print(\"맥락의 형상:\",contexts.shape,'\\n')\n",
    "print(target)\n",
    "print(\"타깃의 형상:\",target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원핫 표현으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:57.997975Z",
     "start_time": "2020-01-06T06:25:57.978209Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    \n",
    "    '''one-hot encoding 으로 변환 \n",
    "    \n",
    "    param corpus: 단어 ID목록(1차원 혹은 2차원의 NumPy배열)\n",
    "    param vocab_size: 어휘수(unique)\n",
    "    :return: one-hot표현(2차원 혹은 3차원의 NumPy배열)\n",
    "    ''' \n",
    "    N = corpus.shape[0]\n",
    "    \n",
    "    if corpus.ndim == 1: # target\n",
    "        one_hot = np.zeros((N, vocab_size), dtype = np.int32) # unique한 어휘 개수로 one hot length 부여됨.\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1 # 한 단어 당 하나의 one-hot\n",
    "            \n",
    "    elif corpus.ndim == 2: # contexts\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype = np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids): # word_id 개수만큼 다시 반복분돌기\n",
    "                one_hot[idx_0,idx_1,word_id] = 1\n",
    "                \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:58.143254Z",
     "start_time": "2020-01-06T06:25:58.121868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 1 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target)\n",
    "convert_one_hot(target, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T06:25:58.306973Z",
     "start_time": "2020-01-06T06:25:58.299962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0]],\n",
       "\n",
       "       [[0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1]]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(contexts)\n",
    "convert_one_hot(contexts, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:30:48.208641Z",
     "start_time": "2020-01-06T15:30:48.039467Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size = 1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:13:21.373279Z",
     "start_time": "2020-01-06T16:13:21.343367Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size # 인수로 어휘 수와 은닉층의 뉴런 수를 받는다.\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f') # 32비트 부동소수점 수\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in) # 입력 측의 맥락을 처리하는 MatMul 계층은 contexts의 개수만큼 생성 (즉, window_size*2 만큼 생성)\n",
    "        self.out_layer = MatMul(W_out) # 출력 측의 MatMul 계층 하나\n",
    "        self.loss_layer = SoftmaxWithLoss() # Softmax with Loss 계층 하나\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "    \n",
    "    # 신경망의 순전파 메서드\n",
    "    # 인수로 맥락과 타깃을 받아서 loss를 반환\n",
    "    def forward(self, contexts, target): \n",
    "        # contexts.shape = (6, 2, 7)\n",
    "        # 0번째 차원의 원소 수 : 미니배치의 수 1번째 차원의 원소 수 : 맥락의 윈도우 크기, 2번째 차원 : 원핫벡터\n",
    "        \n",
    "        # target.shape = (6, 7)\n",
    "        h0 = self.in_layer0.forward(contexts[:,0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "    \n",
    "    # 신경망의 역전파 \n",
    "    def backward(self, dout=1): # 1에서 시작\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T15:41:33.229019Z",
     "start_time": "2020-01-06T15:41:33.222222Z"
    }
   },
   "source": [
    "## 학습 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:17:37.609395Z",
     "start_time": "2020-01-06T16:17:36.555634Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 2 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 3 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 4 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 5 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 6 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 7 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 8 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 9 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 10 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 11 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 12 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 13 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 14 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 15 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 16 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 17 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 18 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 19 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 20 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 21 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 22 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 23 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 24 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 25 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 26 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 27 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 28 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 29 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 30 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 31 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 32 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 33 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 34 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 35 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 36 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 37 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 38 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 39 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 40 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 41 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 42 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 43 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 44 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 45 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 46 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 47 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 48 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 49 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 50 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 51 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 52 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 53 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 54 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 55 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 56 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 57 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 58 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 59 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 60 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 61 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 62 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 63 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 64 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 65 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 66 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 67 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 68 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 69 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 70 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 71 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 72 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 73 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 74 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 75 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 76 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 77 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 78 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 79 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 80 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
      "| epoch 81 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 82 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 83 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 84 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 85 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 86 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 87 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 88 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 89 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 90 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 91 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 92 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
      "| epoch 93 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
      "| epoch 94 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 95 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 96 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
      "| epoch 97 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 98 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
      "| epoch 99 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 100 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 102 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 103 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 104 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 105 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
      "| epoch 106 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 107 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
      "| epoch 108 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
      "| epoch 109 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
      "| epoch 110 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 111 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 112 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
      "| epoch 113 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
      "| epoch 114 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
      "| epoch 115 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
      "| epoch 116 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 117 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 118 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
      "| epoch 119 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
      "| epoch 120 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 121 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 122 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
      "| epoch 123 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 124 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
      "| epoch 125 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
      "| epoch 126 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 127 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
      "| epoch 128 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
      "| epoch 129 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 130 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
      "| epoch 131 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 132 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
      "| epoch 133 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
      "| epoch 134 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
      "| epoch 135 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
      "| epoch 136 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
      "| epoch 137 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 138 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
      "| epoch 139 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 140 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
      "| epoch 141 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
      "| epoch 142 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 143 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 144 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
      "| epoch 145 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 146 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
      "| epoch 147 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 148 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
      "| epoch 149 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
      "| epoch 150 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 151 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 152 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
      "| epoch 153 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 154 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 155 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
      "| epoch 156 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 157 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
      "| epoch 158 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 159 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
      "| epoch 160 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 161 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 162 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
      "| epoch 163 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 164 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 165 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
      "| epoch 166 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 167 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 168 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
      "| epoch 169 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
      "| epoch 170 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 171 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 172 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
      "| epoch 173 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
      "| epoch 174 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
      "| epoch 175 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 176 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 177 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 178 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 179 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 180 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 181 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 182 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 183 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 184 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 185 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 186 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
      "| epoch 187 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
      "| epoch 188 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 189 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
      "| epoch 190 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 191 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 192 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 193 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 194 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 195 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 196 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 197 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
      "| epoch 198 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 199 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 200 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 202 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 203 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
      "| epoch 204 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 205 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 206 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 207 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 208 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 209 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 210 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
      "| epoch 211 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 212 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 213 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 214 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 215 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 216 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 217 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 218 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 219 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 220 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 221 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 222 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 223 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 224 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 225 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 226 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 227 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 228 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 229 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 230 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
      "| epoch 231 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 232 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 233 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 234 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 235 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 236 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 237 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 238 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 239 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 240 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 241 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 242 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 243 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 244 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 245 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 246 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 247 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 248 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 249 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 250 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 251 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 252 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 253 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 254 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 255 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 256 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 257 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 258 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 259 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 260 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 261 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 262 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 263 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 264 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 265 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 266 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 267 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 268 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 269 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 270 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 271 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 272 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 273 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 274 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 275 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 276 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 277 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 278 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 279 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 280 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 281 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 282 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 283 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 284 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 285 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 286 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 287 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 288 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 289 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 290 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 291 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 292 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 293 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 294 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 295 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 296 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 297 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 298 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 299 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 300 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 302 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 303 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 304 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 305 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 306 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 307 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 308 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 309 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 310 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 311 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 312 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 313 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 314 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 315 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 316 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 317 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 318 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 319 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 320 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 321 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 322 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 323 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 324 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 325 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 326 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 327 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 328 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 329 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 330 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 331 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 332 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 333 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 334 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 335 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 336 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 337 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 338 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 339 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 340 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 341 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 342 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 343 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 344 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 345 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 346 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 347 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 348 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 349 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 350 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 351 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 352 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 353 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 354 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 355 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 356 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 357 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 358 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 359 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 360 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 361 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 362 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 363 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 364 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 365 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 366 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 367 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 368 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 369 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 370 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 371 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 372 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 373 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 374 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 375 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 376 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 377 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 378 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 379 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 380 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 381 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 382 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 383 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 384 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 385 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 386 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 387 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 388 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 389 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 390 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 391 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 392 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 393 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 394 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 395 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 396 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 397 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 398 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 399 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 400 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 402 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 403 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 404 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 405 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 406 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 407 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 408 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 409 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 410 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 411 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 412 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 413 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 414 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 415 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 416 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 417 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 418 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 419 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 420 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 421 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 422 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 423 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 424 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 425 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 426 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 427 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 428 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 429 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 430 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 431 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 432 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 433 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 434 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 435 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 436 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 437 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 438 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 439 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 440 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 441 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 442 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 443 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 444 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 445 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 446 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 447 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 448 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 449 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 450 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 451 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 452 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 453 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 454 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 455 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 456 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 457 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 458 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 459 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 460 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 461 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 462 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 463 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 464 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 465 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 466 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 467 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 468 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 469 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 470 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 471 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 472 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 473 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 474 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 475 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 476 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 477 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 478 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 479 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 480 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 481 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 482 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 483 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 484 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 485 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 486 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 487 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 488 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 489 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 490 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 491 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 492 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 493 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 494 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 495 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 496 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 497 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 498 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 499 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 500 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 502 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 503 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 504 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 505 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 506 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 507 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 508 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 509 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 510 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 511 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 512 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 513 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 514 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 515 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 516 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 517 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 518 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 519 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 520 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 521 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 522 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 523 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 524 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 525 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 526 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 527 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 528 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 529 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 530 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 531 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 532 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 533 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 534 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 535 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 536 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 537 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 538 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 539 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 540 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 541 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 542 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 543 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 544 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 545 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 546 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 547 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 548 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 549 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 550 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 551 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 552 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 553 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 554 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 555 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 556 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 557 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 558 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 559 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 560 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 561 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 562 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 563 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 564 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 565 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 566 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 567 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 568 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 569 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 570 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 571 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
      "| epoch 572 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 573 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 574 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 575 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 576 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 577 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 578 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 579 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 580 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 581 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 582 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 583 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 584 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 585 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 586 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 587 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 588 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 589 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 590 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 591 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 592 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 593 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 594 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 595 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 596 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 597 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 598 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 599 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 600 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 602 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 603 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 604 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 605 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 606 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 607 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 608 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 609 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 610 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 611 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 612 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 613 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 614 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 615 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 616 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 617 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 618 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 619 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 620 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 621 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 622 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 623 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 624 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 625 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 626 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 627 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 628 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 629 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 630 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 631 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 632 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 633 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 634 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 635 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 636 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 637 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 638 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 639 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 640 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 641 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 642 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 643 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 644 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 645 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 646 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 647 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 648 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 649 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 650 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 651 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 652 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 653 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 654 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 655 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 656 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 657 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 658 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 659 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 660 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 661 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 662 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 663 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 664 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 665 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 666 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 667 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 668 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 669 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 670 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 671 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 672 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
      "| epoch 673 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 674 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 675 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 676 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
      "| epoch 677 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 678 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 679 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 680 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 681 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 682 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 683 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 684 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 685 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 686 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 687 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 688 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 689 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 690 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 691 |  iter 1 / 2 | time 0[s] | loss 0.36\n",
      "| epoch 692 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 693 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 694 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 695 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 696 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
      "| epoch 697 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 698 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 699 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 700 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 702 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 703 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 704 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 705 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 706 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 707 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 708 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 709 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 710 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 711 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 712 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 713 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 714 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 715 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 716 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 717 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 718 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 719 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 720 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 721 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 722 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 723 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 724 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 725 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 726 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 727 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 728 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 729 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 730 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 731 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 732 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 733 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 734 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 735 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 736 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 737 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 738 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 739 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 740 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 741 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 742 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 743 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
      "| epoch 744 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 745 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 746 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 747 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 748 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 749 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 750 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 751 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 752 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 753 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
      "| epoch 754 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 755 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 756 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 757 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 758 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 759 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 760 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 761 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 762 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 763 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
      "| epoch 764 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 765 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 766 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
      "| epoch 767 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 768 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 769 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 770 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 771 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 772 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 773 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 774 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 775 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 776 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 777 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 778 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 779 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 780 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
      "| epoch 781 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 782 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 783 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 784 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 785 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
      "| epoch 786 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 787 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
      "| epoch 788 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 789 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 790 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 791 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 792 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 793 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 794 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 795 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 796 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 797 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 798 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 799 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 800 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 802 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 803 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 804 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 805 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 806 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 807 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 808 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 809 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 810 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 811 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 812 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 813 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 814 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 815 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 816 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 817 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 818 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 819 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 820 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 821 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 822 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 823 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 824 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 825 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 826 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 827 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 828 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 829 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 830 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 831 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 832 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 833 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 834 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 835 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 836 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 837 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 838 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 839 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 840 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 841 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 842 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 843 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 844 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 845 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 846 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 847 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 848 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 849 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
      "| epoch 850 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 851 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 852 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 853 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 854 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 855 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 856 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 857 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 858 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
      "| epoch 859 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 860 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 861 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
      "| epoch 862 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 863 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 864 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 865 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 866 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 867 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 868 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 869 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 870 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 871 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 872 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 873 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 874 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 875 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 876 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
      "| epoch 877 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 878 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 879 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 880 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 881 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 882 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 883 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 884 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 885 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 886 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 887 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 888 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 889 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 890 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 891 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 892 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 893 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 894 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
      "| epoch 895 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 896 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 897 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 898 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
      "| epoch 899 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 900 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 901 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 902 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 903 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 904 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 905 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 906 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 907 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 908 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 909 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 910 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 911 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 912 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 913 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 914 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 915 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 916 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 917 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 918 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 919 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 920 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 921 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 922 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 923 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 924 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 925 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 926 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 927 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 928 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
      "| epoch 929 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 930 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 931 |  iter 1 / 2 | time 0[s] | loss 0.31\n",
      "| epoch 932 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 933 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
      "| epoch 934 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 935 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 936 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 937 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 938 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 939 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 940 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
      "| epoch 941 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 942 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 943 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 944 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 945 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 946 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 947 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 948 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 949 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 950 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 951 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 952 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 953 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 954 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 955 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 956 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 957 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 958 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 959 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 960 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 961 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 962 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 963 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 964 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 965 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 966 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 967 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 968 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 969 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
      "| epoch 970 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 971 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 972 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 973 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 974 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 975 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 976 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 977 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 978 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
      "| epoch 979 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 980 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 981 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
      "| epoch 982 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 983 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 984 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 985 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 986 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
      "| epoch 987 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 988 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 989 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 990 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 991 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 992 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 993 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 994 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 995 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 996 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 997 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 998 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 999 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 1000 |  iter 1 / 2 | time 0[s] | loss 0.41\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target,vocab_size)\n",
    "contexts = convert_one_hot(contexts,vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam() # 매개변수 갱신 방법의 한 종류\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가로축은 학습 횟수, 세로축은 손실\n",
    "\n",
    "학습이 경과될수록 손실이 줄어드는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:19:34.022993Z",
     "start_time": "2020-01-06T16:19:34.015611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [ 1.3351159 -1.1248811  1.1296217 -1.1195978 -1.0375124]\n",
      "\n",
      "say [-0.01697864  1.1917418  -1.1481713   1.1842325   1.2066556 ]\n",
      "\n",
      "goodbye [ 0.61621106 -0.75953335  0.81836444 -0.70477396 -0.8977362 ]\n",
      "\n",
      "and [-1.8688438   1.0109028  -1.097344    0.9846056   0.96556264]\n",
      "\n",
      "i [ 0.6094434  -0.76561135  0.8301478  -0.71917397 -0.8725316 ]\n",
      "\n",
      "hello [ 1.3669877 -1.1133312  1.1342381 -1.1258147 -1.0214459]\n",
      "\n",
      ". [ 1.6230313   1.0678545  -0.64506924  1.0918348   1.0815095 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습이 끝난 뒤의 가중치 매개변수는 인스턴스 변수 word_vecs에 담겨있음\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word,word_vecs[word_id],end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 행에는 대응하는 단어 ID에 대한 분산 표현이 저장되어 있다. 단어를 밀집벡터로 표현완료."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec 보충"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델과 확률\n",
    "\n",
    "사후확률 $P(A|B)$는 `B가 주어졌을 때 A가 일어날 확률`이다.\n",
    "\n",
    "$w_1  w_2 .... w_{t-1}  w_{t}  w_{t+1} ... $\n",
    "\n",
    "이것을 word2vec에 대입하면, <u>맥락 $w_{t-1}과 w_{t+1}$이 주어졌을 때 타깃이 $w_{t}$가 될 확률</u>은 수식으로 $P(w_{t}|w_{t-1},w_{t+1})$ 으로 나타날 수 있다. **====> CBOW**\n",
    "\n",
    "또한 CBOW 모델의 손실 함수도 간결하게 표현할 수 있다.\n",
    "\n",
    "1장에서의 교차 엔트로피 오차 식 : $ L = -\\sum_{k} t_klog{y_k} $, $t_k$는 k번째 클래스에 해당하는 정답 레이블\n",
    "\n",
    "수식에서 문제의 정답은 \"$w_{t}$의 발생\" 이기 때문에 $w_{t}$에 해당하는 원소만 1이고 나머지는 0이 된다. 이점을 감안하면 다음의 손실 식이 도출된다.\n",
    "\n",
    "$$ L = -logP(w_{t}|w_{t-1},w_{t+1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip-gram 모델\n",
    "\n",
    "skip-gram은 CBOW에서 다루는 맥락과 타깃을 역전시킨 모델이다.\n",
    "\n",
    "**CBOW 모델은 맥락이 여러 개 있고, 그 여러 맥락으로부터 중앙의 단어(타깃)을 추측한다.**\n",
    "\n",
    "     you ? goodbye and I say hello.\n",
    "\n",
    "**한편, skip-gram 모델은 중앙의 단어(타깃)으로부터 주변의 여러 단어(맥락)을 추측한다.**\n",
    "\n",
    "     ? say ? and I say hello.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/skipgram.png\" width=\"250\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW와 반대로, skip-gram의 입력층은 하나이고 출력층은 맥락의 수만큼 존재한다. 각 출력층에서는 개별적으로 손실을 구하고, 이 개별 손실들을 모두 더한 값을 최종 손실로 한다.\n",
    "\n",
    "마찬가지로 skip-gram 모델을 확률 표기로 나타내면, <u>타깃 $w_{t}$가 주어졌을 때 $w_{t-1}$와 $w_{t+1}$가 동시에 일어날 확률을 뜻한다.</u>\n",
    "\n",
    "$$P(w_{t-1},w_{t+1}|w_{t})$$\n",
    "\n",
    "skip-gram 모델에서는 맥락의 단어들 사이에 관련성이 없다고 가정하고 다음과 같이 분해한다. (조건부 독립)\n",
    "\n",
    "$$P(w_{t-1},w_{t+1}|w_{t}) = P(w_{t-1}|w_{t})P(w_{t+1}|w_{t})$$\n",
    "\n",
    "이어서 skip-gram의 손실 함수를 유도하자.\n",
    "\n",
    "$ L = -logP(w_{t-1},w_{t+1}|w_{t})$ \n",
    "\n",
    "$ =-logP(w_{t-1}|w_{t})P(w_{t+1}|w_{t}) $\n",
    "\n",
    "$ = -(logP(w_{t-1}|w_{t}) + log(P(w_{t+1}|w_{t}))$\n",
    "\n",
    "식에서도 알 수 있듯, skip-gram 모델의 손실 함수는 맥락 별로 손실을 구한 다음에 모두 더한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW vs. skip-gram\n",
    "\n",
    "둘 중 어느 모델을 사용해야 할까? **답은 skip-gram 모델이다.**\n",
    "\n",
    "단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋은 경우가 많다. 특히, 말뭉치가 커질수록 이러한 경향성은 커진다. \n",
    "\n",
    "반면, 학습 속도 면에서는 CBOW 모델이 더 빠르다.skip-gram 모델은 손실을 맥락의 수만큼 구해야 해서 계산 비용이 그만큼 커지게 된다. \n",
    "\n",
    "skip-gram 모델은 하나의 단어로부터 그 주변 단어를 예측. skip-gram 모델이 CBOW 모델보다 더 어려운 문제를 예측한다고 볼 수 있다. 더 어려운 상황에서 학습하는 만큼 skip-gram 모델로부터 생성된 단어의 분산 표현이 더 정교하고 뛰어날 가능성이 커진다고도 생각해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "# skip-gram 구현\n",
    "class SimpleSkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer1 = SoftmaxWithLoss() # CBOW와는 반대로 출력층에 맥락의 개수만큼 레이어 생성\n",
    "        self.loss_layer2 = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 단어의 분산 표현 저장\n",
    "        self.word_vecs = W_in # skip-gram에서도 동일하게 인풋 가중치 사용\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "        s = self.out_layer.forward(h)\n",
    "        l1 = self.loss_layer1.forward(s, contexts[:, 0])\n",
    "        l2 = self.loss_layer2.forward(s, contexts[:, 1])\n",
    "        loss = l1 + l2 # 모든 맥락의 손실을 합친 것이 skip-gram의 손실\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dl1 = self.loss_layer1.backward(dout)\n",
    "        dl2 = self.loss_layer2.backward(dout)\n",
    "        ds = dl1 + dl2\n",
    "        dh = self.out_layer.backward(ds)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계 기반 vs. 추론 기반\n",
    "\n",
    "**1. 어휘에 추가할 새 단어가 생겨서 단어의 분산 표현을 갱신해야 하는 상황**\n",
    "\n",
    "통계 기반 기법 : 계산을 처음부터 다시 해야한다. 단어의 분산 표현을 조금만 수정하고 싶어도, 동시발생 행렬을 다시 만들고 SVD를 수행하는 일련의 작업을 다시 해야 한다. \n",
    "\n",
    "추론 기반 기법 : 매개변수를 다시 학습할 수 있다. 지금까지 학습한 가중치를 초깃값으로 사용해 다시 학습하면 되는데, 이런 특성 덕분에 기존에 학습한 경험을 해치지 않으면서 단어의 분산 표현을 효율적으로 갱신할 수 있다.\n",
    "\n",
    "\n",
    "**2. 두 기법으로 얻는 단어의 분산 표현의 성격이나 정밀도 측면**\n",
    "\n",
    "통게 기반 기법 : 주로 단어의 유사성이 인코딩된다. \n",
    "\n",
    "추론 기반 기법 : 단어의 유사성 뿐만 아니라, 그리고 복잡한 단어 사이의 패턴도 파악되어 인코딩된다.\n",
    "\n",
    "BUT, 실제로 단어의 유사성을 정량적으로 평가해본 연구 결과에서, 추론 기반과 통계 기반 기법의 우열을 가릴 수 없었다.\n",
    "\n",
    "**두 기법은 사실 상 연관되어 있다**\n",
    "\n",
    "구체적으로는 skip-gram + negative sampling을 이용한 모델은 동시발생 행렬에 특수한 행렬 분해를 적용한 것과 같다.\n",
    "\n",
    "**GloVe**\n",
    "\n",
    "==> 추론 기반 기법 + 통계 기반 기법\n",
    "\n",
    "말뭉치 전체의 통계 정보를 손실 함수에 도입해 미니배치 학습을 하는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
